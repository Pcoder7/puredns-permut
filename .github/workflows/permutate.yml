name: Distributed Permutation Enumeration

on:
  workflow_dispatch: {}

permissions:
  contents: write   # For creating matrix.json, artifacts, and potentially committing aggregated results
  actions: write    # For triggering workflows in Account 2's repository

env:
  # --- Chunking and Distribution Configuration ---
  LINES_PER_CHUNK: 25000 # Unchanged from original
  DISTRIBUTION_THRESHOLD: 20    # NEW: The minimum total chunks to trigger 3-way distribution
  PRIMARY_PERCENTAGE: 40        # NEW: 30% for the primary account
  SECONDARY_PERCENTAGE: 35      # NEW: 30% for the secondary account

  # Define Account 2 repo details here or get from secrets for more flexibility
  # --- Account 2 Details ---
  ACCOUNT2_REPO_OWNER: ${{ secrets.ACCOUNT2_REPO_OWNER || 'pushrockzz' }} # Unchanged from original
  ACCOUNT2_REPO_NAME: ${{ secrets.ACCOUNT2_REPO_NAME || 'puredns-resolve' }} # Unchanged from original

  # --- Account 3 Details ---
  ACCOUNT3_REPO_OWNER: ${{ secrets.ACCOUNT3_REPO_OWNER || 'Sari2705' }}      # NEW
  ACCOUNT3_REPO_NAME: ${{ secrets.ACCOUNT3_REPO_NAME || 'puredns-resolve' }} # NEW

jobs:
  prepare_all_chunks_and_package:
    name: Prepare All Chunks & Package
    runs-on: ubuntu-latest
    container:
      image: ghcr.io/pcoder7/spider-puredns-actions:latest
      credentials:
        username: ${{ secrets.GHCR_USER }}
        password: ${{ secrets.GHCR_TOKEN }}
    outputs:
      # Output 1: The full JSON matrix string of ALL generated chunks
      all_chunks_matrix_json: ${{ steps.build_full_matrix.outputs.full_matrix }}
      # Output 2: The total number of chunks generated
      total_chunks_count: ${{ steps.build_full_matrix.outputs.total_chunks }}
      # Output 3: The name of the artifact containing all chunks and resolvers
      chunk_package_artifact_name: "all-chunks-package-${{ github.run_id }}"
    if: ${{ github.actor == 'Pcoder7' }}
    steps:
      - name: Log trigger info
        run: |
          echo "Triggered by: ${{ github.actor }}"
          echo "Run id: ${{ github.run_id }}"
          
      - name: Checkout repository
        uses: actions/checkout@v3
        with:
          token: ${{ secrets.GITHUB_TOKEN }}
          fetch-depth: 0
          
      - name: Setup Go
        uses: actions/setup-go@v5
        with:
          go-version: '1.23'

      - name: Cache Go modules & binaries
        uses: actions/cache@v3
        with:
          path: |
            $HOME/go/pkg/mod
            ~/.cache/go-build
            $HOME/go/bin
          key: ${{ runner.os }}-go-cache-${{ github.ref_name }}
          restore-keys: |
            ${{ runner.os }}-go-cache-
               
      - name: Install Tools
        run: |
          # Installing dsieve 
          if ! command -v dsieve >/dev/null; then
            echo "Installing smap…"
            go install github.com/trickest/dsieve@latest
          else
            echo "dsieve already in cache"
          fi    

          if ! command -v anew >/dev/null; then
            echo "Installing anew…"
            go install -v github.com/tomnomnom/anew@latest
          else
            echo "anew already in cache"
          fi
        
          echo "$HOME/go/bin" >> $GITHUB_PATH

      - name: Build Smart Tiered & Randomized Seed List from Store-Recon
        id: create_final_list
        shell: bash
        env:
          STORE_RECON_PAT: ${{ secrets.PAT_FOR_SECONDRY }}
          STORE: ${{ secrets.STORE }}
          ACCOUNT2_USERNAME: ${{ secrets.ACCOUNT2_REPO_OWNER }}
        run: |
          # Robust header for script safety
          set -euo pipefail
          IFS=$'\n\t'
          TMPDIR=${TMPDIR:-/tmp}

          echo "INFO: Performing a fast, shallow clone of the results repository..."
          git clone --depth 1 "https://x-access-token:${STORE_RECON_PAT}@github.com/${ACCOUNT2_USERNAME}/${STORE}.git" store-recon-temp

          echo "INFO: Aggregating and aggressively sanitizing all results into a master list..."
          if [ -d "store-recon-temp/results" ]; then
                find store-recon-temp/results -type f -name "puredns_result.txt" -exec awk '{print $1}' {} + \
                  | sed 's/[^[:graph:][:space:]]//g' \
                  | tr -s '[:space:]' '\n' \
                  | sed '/^$/d' \
                  > all-resolved-subdomains.txt
          else
                echo "WARNING: 'results' directory not found. Exiting."
                exit 0
          fi

          if [ ! -s all-resolved-subdomains.txt ]; then
                echo "WARNING: Master list 'all-resolved-subdomains.txt' is empty. Exiting."
                exit 0
          fi

          echo "INFO: Extracting unique root domains from the master list..."
          # Use locale C for a significant performance boost
          dsieve -if all-resolved-subdomains.txt -f 2 | LC_ALL=C sort -u > unique_root_domains.txt

          > final-subdomain-list.txt
          echo "INFO: Building smart seed list for each root domain..."
          while read -r ROOT_DOMAIN; do
            if [ -z "$ROOT_DOMAIN" ]; then continue; fi
            echo "====================================================="
            echo "Processing Root Domain: $ROOT_DOMAIN"

            TEMP_ROOT_LIST="${TMPDIR}/${ROOT_DOMAIN}-list.txt"
            grep -a -E "(^|\\.)${ROOT_DOMAIN//./\\.}(\$)" all-resolved-subdomains.txt --color=never > "$TEMP_ROOT_LIST" || true

            if [ ! -s "$TEMP_ROOT_LIST" ]; then
                echo "WARNING: No subdomains found for '$ROOT_DOMAIN' after filtering. Skipping."
                continue
            fi
            
            echo "  -> Pre-calculating top 5000 subdomains to optimize performance..."
            TEMP_TOP_5000_LIST="${TMPDIR}/${ROOT_DOMAIN}-top5000.txt"
            dsieve -if "$TEMP_ROOT_LIST" -f 3 -top 5000 > "$TEMP_TOP_5000_LIST"
            
            TEMP_SEED_LIST_DOMAIN="${TMPDIR}/${ROOT_DOMAIN}-seeds.txt"
            > "$TEMP_SEED_LIST_DOMAIN"

            echo "  -> Getting a random 1000 sample from the top 2000..."
            head -n 2000 "$TEMP_TOP_5000_LIST" | shuf -n 1000 >> "$TEMP_SEED_LIST_DOMAIN"

            echo "  -> Getting a random 500 sample from the 'mid-tier' (ranks 2001-5000)..."
            tail -n +2001 "$TEMP_TOP_5000_LIST" | shuf -n 500 >> "$TEMP_SEED_LIST_DOMAIN"

            # Use locale C for a significant performance boost
            LC_ALL=C sort -u "$TEMP_SEED_LIST_DOMAIN" -o "$TEMP_SEED_LIST_DOMAIN"

            SEED_COUNT=$(wc -l < "$TEMP_SEED_LIST_DOMAIN")
            echo "  -> Generated $SEED_COUNT potential seeds for $ROOT_DOMAIN."
            cat "$TEMP_SEED_LIST_DOMAIN" >> final-subdomain-list.txt

            rm "$TEMP_ROOT_LIST" "$TEMP_TOP_5000_LIST"

          done < unique_root_domains.txt

          echo "INFO: Performing final de-duplication of the master seed list..."
          # Use locale C for a significant performance boost
          LC_ALL=C sort -u final-subdomain-list.txt -o final-subdomain-list.txt

          FINAL_COUNT=$(wc -l < final-subdomain-list.txt)
          echo "====================================================="
          echo "SUCCESS: 'final-subdomain-list.txt' created with $FINAL_COUNT total unique seeds for permutation."

          rm -rf store-recon-temp                
      
      - name: Process, Generate, and Filter Permutations with Dynamic Wordlists
        env:
          MASTER_INPUT_LIST: "final-subdomain-list.txt" # More flexible way to set the input file
        shell: bash
        run: |
          # --- Setup, Input Files, and Tool Installation ---
          export TMPDIR="$RUNNER_TEMP"
          set -e # Exit immediately if a command exits with a non-zero status.

          echo "INFO: Verifying input file..."
          [ -f "$MASTER_INPUT_LIST" ] || { echo "CRITICAL: Missing input file '$MASTER_INPUT_LIST'."; exit 0; }

          

          echo "-----------------------------------------------------"

          # --- PRE-PROCESSING: Extract Unique Root Domains ---
          echo "PRE-PROCESSING: Extracting unique root domains from '$MASTER_INPUT_LIST'..."
          dsieve -if "$MASTER_INPUT_LIST" -f 2 | sort -u > unique_root_domains.txt
          ROOT_DOMAIN_COUNT=$(wc -l < unique_root_domains.txt)
          echo "INFO: Found $ROOT_DOMAIN_COUNT unique root domains to process."
          if [ "$ROOT_DOMAIN_COUNT" -eq 0 ]; then
            echo "WARNING: No root domains found. Exiting."
            touch subdomains.txt # Ensure file exists for subsequent steps
            exit 0
          fi
          echo "-----------------------------------------------------"



          # This file will hold all generated permutations before unique filtering
          > all_permutations_temp.txt

          # --- MAIN PROCESSING LOOP: Iterate over each root domain ---
          while read -r ROOT_DOMAIN; do
            if [ -z "$ROOT_DOMAIN" ]; then continue; fi
            echo "====================================================="
            echo "PROCESSING ROOT DOMAIN: $ROOT_DOMAIN"
            echo "====================================================="

            # Step 1: Create a temporary base list for the current root domain
            TEMP_BASE_LIST="${TMPDIR}/${ROOT_DOMAIN}-base.txt"
            echo "INFO: Creating temporary base list for '$ROOT_DOMAIN'..."
            grep -E "(^|\\.)${ROOT_DOMAIN//./\\.}(\$)" "$MASTER_INPUT_LIST" --color=never > "$TEMP_BASE_LIST" || true

            if [ ! -s "$TEMP_BASE_LIST" ]; then
                echo "WARNING: No subdomains found for '$ROOT_DOMAIN' in the master list. Skipping."
                continue
            fi

            # Step 2: Generate a dynamic, domain-specific wordlist
            DYNAMIC_WORDLIST="${TMPDIR}/${ROOT_DOMAIN}-wordlist.txt"
            echo "INFO: Generating dynamic wordlist for '$ROOT_DOMAIN'..."
            # This pipeline is efficient and correct
            dsieve -if "$TEMP_BASE_LIST" -f 3 -top 50| awk -F '[-.]' '{for(i=1;i<=NF;i++) print $i}' | sort -u | grep -vE '^[0-9]+$|^.$' --color=never | anew -q "$DYNAMIC_WORDLIST" || true
            
            #dsieve -if "$TEMP_BASE_LIST" -f 4 -top 500 | awk -F '[-.]' '{for(i=1;i<=NF;i++) print $i}' | sort -u | grep -vE '^[0-9]+$|^.$' | anew -q "$DYNAMIC_WORDLIST"
            #dsieve -if "$TEMP_BASE_LIST" -f 5 -top 200 | awk -F '[-.]' '{for(i=1;i<=NF;i++) print $i}' | sort -u | grep -vE '^[0-9]+$|^.$' | anew -q "$DYNAMIC_WORDLIST"
            
            DYNAMIC_WORD_COUNT=$(wc -l < "$DYNAMIC_WORDLIST")
            if [ "$DYNAMIC_WORD_COUNT" -eq 0 ]; then
                echo "WARNING: Could not generate a dynamic wordlist for '$ROOT_DOMAIN'. Skipping permutation tools."
                continue
            fi
            echo "INFO: Generated dynamic wordlist with $DYNAMIC_WORD_COUNT words."

            # Step 3: Run permutation tools efficiently, once per tool per domain

                
                echo "  -> Running gotator with dynamic wordlist..."
                # THE FIX: Build the command string first to handle quotes correctly.
                # We use 'printf' with '%q' to safely quote the wordlist path.
                SAFELY_QUOTED_WORDLIST=$(printf '%q' "$DYNAMIC_WORDLIST")


                # Stage 3a: Dnsgen
                echo "  -> Running dnsgen with dynamic wordlist..."
                DNSGEN_CMD="dnsgen -w $SAFELY_QUOTED_WORDLIST {} > {}-dnsgen.txt"

                chrunk -p 20 -i "$TEMP_BASE_LIST" -c 20 -prefix "${ROOT_DOMAIN}-dnsgen" -cmd "$DNSGEN_CMD"
                
                DNSGEN_RESULTS_TMP="${TMPDIR}/${ROOT_DOMAIN}-dnsgen-results.txt"
                cat "$TMPDIR"/chrunk-data/${ROOT_DOMAIN}-dnsgen-*.txt 2>/dev/null > "$DNSGEN_RESULTS_TMP"
                DNSGEN_UNIQUE_ADDITIONS=$(cat "$DNSGEN_RESULTS_TMP" | sort -u | wc -l)
                echo "    - Dnsgen generated $DNSGEN_UNIQUE_ADDITIONS found new unique subdomains."
                cat "$DNSGEN_RESULTS_TMP" | anew -q all_permutations_temp.txt
                
                # Stage 3b: Alterx
                echo "  -> Running alterx with dynamic wordlist..."
               
                # 'printf' also works for the alterx template path
                SAFELY_QUOTED_TEMPLATE=$(printf '%q' "template.txt")
                ALTERX_CMD="alterx -l {} -enrich -p $SAFELY_QUOTED_TEMPLATE -pp 'word=$SAFELY_QUOTED_WORDLIST' > {}-alterx.txt"
                
                chrunk -p 20 -i "$TEMP_BASE_LIST" -c 20 -prefix "${ROOT_DOMAIN}-alterx" -cmd "$ALTERX_CMD"

                ALTERX_RESULTS_TMP="${TMPDIR}/${ROOT_DOMAIN}-alterx-results.txt"
                cat "$TMPDIR"/chrunk-data/${ROOT_DOMAIN}-alterx-*.txt 2>/dev/null > "$ALTERX_RESULTS_TMP"
                ALTERX_UNIQUE_ADDITIONS=$(cat "$ALTERX_RESULTS_TMP" | anew -d all_permutations_temp.txt | wc -l)
                echo "    - Alterx found $ALTERX_UNIQUE_ADDITIONS new unique subdomains."
                cat "$ALTERX_RESULTS_TMP" | anew -q all_permutations_temp.txt

                # Stage 3b: Alterx
                
                #GOTATOR_CMD="gotator-solved -sub {} -depth 1 -adv -perm '"$DYNAMIC_WORDLIST"' -mindup -silent | anew -q {}-gotator.txt"
                
                chrunk -p 20 -i "$TEMP_BASE_LIST" -c 20 -cmd 'gotator-solved -sub "{}" -depth 1 -perm '"$DYNAMIC_WORDLIST"' -adv -mindup -silent | anew -q "{}-gotator.txt"' -prefix "${ROOT_DOMAIN}-gotator"
                
                GOTATOR_RESULTS_TMP="${TMPDIR}/${ROOT_DOMAIN}-gotator-results.txt"
                cat "$TMPDIR"/chrunk-data/${ROOT_DOMAIN}-gotator-*.txt 2>/dev/null > "$GOTATOR_RESULTS_TMP"
                
                wc -l "$GOTATOR_RESULTS_TMP"
                GOTATOR_COUNT=$(cat "$GOTATOR_RESULTS_TMP" | anew -d all_permutations_temp.txt | wc -l)
                echo "    - Gotator found $GOTATOR_COUNT new unique subdomains"
                cat "$GOTATOR_RESULTS_TMP" | anew -q all_permutations_temp.txt
              
            
            echo "INFO: Finished processing for $ROOT_DOMAIN."
            
            echo "INFO: Finished processing for $ROOT_DOMAIN."
            # Clean up chrunk's temporary output directories for this root domain
            
          done < unique_root_domains.txt

          # Now, de-duplicate all results at once
          echo "INFO: De-duplicating all generated permutations..."
          cat all_permutations_temp.txt | anew -q raw_permutations.txt
          rm all_permutations_temp.txt

          echo "====================================================="

          # --- FINAL FILTERING STAGE ---
          echo "FINAL FILTERING: Removing jargon and complex numeric/IP-like patterns..."

          # First, ensure the files we need actually exist
          if [ ! -s "raw_permutations.txt" ]; then
            echo "WARNING: raw_permutations.txt is empty or does not exist. No filtering will be done."
            touch subdomains.txt # Create an empty file to ensure downstream steps don't fail
            exit 0
          fi
          if [ ! -f "filter.txt" ]; then
            echo "CRITICAL ERROR: filter.txt not found. Cannot apply filters. Aborting."
            exit 0
          fi

          RAW_COUNT=$(wc -l < raw_permutations.txt)
          echo "INFO: Starting with $RAW_COUNT raw permutations."

          # --- THE FIX: Sanitize and apply the filter file ---
          echo "INFO: Sanitizing filter.txt to remove empty lines or Windows line endings..."
          
          # This creates a clean version of the filter file that grep can safely use.
          # It removes any empty lines and converts potential \r\n to \n.
          
          grep -v '^$' filter.txt --color=never  | sed 's/\r$//' > sanitized_filters.txt || true

          echo "INFO: Applying filters from sanitized_filters.txt..."

          grep -ivE -f sanitized_filters.txt raw_permutations.txt --color=never > subdomains.txt || true

          # Clean up the temporary sanitized file
          rm sanitized_filters.txt

          FINAL_COUNT=$(wc -l < subdomains.txt)
          FILTERED_COUNT=$((RAW_COUNT - FINAL_COUNT))

          echo "INFO: Filtered out $FILTERED_COUNT subdomains."
          echo "FINAL: The cleaned 'subdomains.txt' now contains $FINAL_COUNT permutations."
          rm -rf "$TMPDIR"/chrunk-data/${ROOT_DOMAIN}-*
          cat raw_permutations.txt | anew -q subdomains.txt
          wc -l subdomains.txt
          
      - name: DEBUG - Verify Tools and Initial File System State
        shell: bash
        run: |
          echo "DEBUG: Current directory: $(pwd)"
          echo "DEBUG: Listing all files in repository root and subdirectories:"
          find . -print
          echo "-----------------------------------------------------"
          echo "DEBUG: Specifically looking for subdomains.txt files:"
          find . -type f -name 'subdomains.txt' -print || echo "DEBUG: 'find' for subdomains.txt failed or found nothing."
          echo "-----------------------------------------------------"
          echo "DEBUG: Checking for 'jq' command..."
          if ! command -v jq &> /dev/null; then
            echo "CRITICAL_ERROR: 'jq' command not found. This is essential. Aborting."
            exit 0
          else
            echo "DEBUG: jq version: $(jq --version)"
          fi
          echo "-----------------------------------------------------"
          echo "DEBUG: Checking for 'split' command..."
          if ! command -v split &> /dev/null; then
            echo "CRITICAL_ERROR: 'split' command not found. This is essential. Aborting."
            exit 0
          else
            echo "DEBUG: split version information (if available):"
            split --version || echo "DEBUG: Could not get split version."
          fi
          echo "-----------------------------------------------------"
      - name: Build Full Matrix & Create Chunks
        id: build_full_matrix
        shell: bash
        run: |
          # --- Start of Script ---
          JSON_MATRIX='[]' # Initialize as an empty JSON array string
          echo "INFO: Initializing JSON_MATRIX as: '$JSON_MATRIX'"
          echo "INFO: Locating 'subdomains.txt' files..."
          find . -type f -name "subdomains.txt" -print0 > found_files.tmp
          declare -a files=() # Explicitly declare as an array
          while IFS= read -r -d $'\0' file_path_from_find; do
            standardized_file_path=$(echo "$file_path_from_find" | sed 's|^\./||')
            files+=("$standardized_file_path")
          done < found_files.tmp
          rm found_files.tmp # Clean up temporary file
          if [ "${#files[@]}" -eq 0 ]; then
            echo "WARNING: No 'subdomains.txt' files found in the repository."
            echo "INFO: JSON_MATRIX will remain '[]'. No chunks will be generated."
          else
            echo "INFO: Found ${#files[@]} 'subdomains.txt' file(s):"
            printf "  => '%s'\n" "${files[@]}" # Print each found file for clarity
            for file_path in "${files[@]}"; do
              echo "-----------------------------------------------------"
              echo "INFO: Processing file: '$file_path'"
              domain_dir=$(dirname "$file_path")
              if [ "$domain_dir" == "." ]; then
                domain=$(basename "$file_path" .txt)
              else
                domain=$(basename "$domain_dir")
              fi
              echo "INFO: Deduced domain: '$domain'"
              if [ ! -f "$file_path" ]; then
                echo "ERROR: File '$file_path' reported by find, but not found now. Skipping."
                continue
              fi
              if [ ! -s "$file_path" ]; then
                echo "WARNING: File '$file_path' is empty. Skipping splitting for this file."
                continue
              fi
              echo "INFO: Creating chunk directory 'chunks/$domain' if it doesn't exist."
              mkdir -p "chunks/$domain"
              output_chunk_prefix="chunks/$domain/chunk_"
              echo "INFO: Splitting '$file_path' into chunks in 'chunks/$domain/' with prefix '$output_chunk_prefix' (max $LINES_PER_CHUNK lines/chunk)"
              
              split -l "$LINES_PER_CHUNK" -a 3 --numeric-suffixes=1 "$file_path" "$output_chunk_prefix"
              split_exit_code=$?
              if [ $split_exit_code -ne 0 ]; then
                echo "ERROR: 'split' command failed with exit code $split_exit_code for file '$file_path'. Skipping chunk generation for this file."
                continue
              fi
              
              CHUNK_COUNT_FOR_THIS_FILE=0
              while IFS= read -r chunk_file_path; do
                if [ -z "$chunk_file_path" ]; then continue; fi
                echo "DEBUG: Processing chunk_file_path: '$chunk_file_path' for domain '$domain'"
                echo "DEBUG: Current JSON_MATRIX before jq (first 100 chars): $(echo "$JSON_MATRIX" | head -c 100)"
                echo "DEBUG: Validating current JSON_MATRIX with jq -e:"
                if echo "$JSON_MATRIX" | jq -e . > /dev/null 2>&1; then
                  echo "DEBUG: Current JSON_MATRIX is VALID."
                else
                  echo "CRITICAL_ERROR_DETECTED: Current JSON_MATRIX is INVALID before processing '$chunk_file_path'."
                  echo "CRITICAL_ERROR_DETECTED: JSON_MATRIX content: $JSON_MATRIX"
                  if [ "$JSON_MATRIX" != "[]" ] && [ "$CHUNK_COUNT_FOR_THIS_FILE" -gt 0 ]; then
                      echo "ABORTING chunk addition for this file due to previously corrupted JSON_MATRIX."
                      break 
                  else
                      echo "WARNING: JSON_MATRIX was invalid but appeared to be at an initial state. Attempting to reset to []."
                      JSON_MATRIX="[]"
                  fi
                fi
                
                if ! command -v jq &> /dev/null; then
                  echo "CRITICAL_ERROR: 'jq' command not found during chunk processing. Aborting this file's chunk addition."
                  break 
                fi
                
                echo "DEBUG: jq command to be run: printf '%s' \"\$JSON_MATRIX\" | jq -c --arg d \"$domain\" --arg c \"$chunk_file_path\" '. + [{domain:\$d,chunk:\$c}]'"
                TEMP_JSON_MATRIX=$(printf '%s' "$JSON_MATRIX" | jq -c --arg d "$domain" --arg c "$chunk_file_path" '. + [{domain:$d,chunk:$c}]')
                jq_exit_code=$?
                if [ $jq_exit_code -ne 0 ]; then
                  echo "ERROR: jq command failed (exit code $jq_exit_code) when trying to add chunk '$chunk_file_path' for domain '$domain'."
                  echo "ERROR: Input JSON_MATRIX was (first 100 chars): $(echo "$JSON_MATRIX" | head -c 100)"
                  echo "ERROR: Arguments to jq were: domain='$domain', chunk_file_path='$chunk_file_path'"
                  continue
                elif [ -z "$TEMP_JSON_MATRIX" ]; then
                  echo "ERROR: jq command produced empty output for chunk '$chunk_file_path'. This should not happen with '. + [...]'."
                  echo "ERROR: Input JSON_MATRIX that led to empty output (first 200 chars): $(echo "$JSON_MATRIX" | head -c 200)"
                  echo "ERROR: Full input JSON_MATRIX that led to empty output: $JSON_MATRIX"
                  echo "ERROR: Arguments to jq were: domain='$domain', chunk_file_path='$chunk_file_path'"
                  echo "DEBUG: Re-validating the input JSON_MATRIX that caused empty output:"
                  if echo "$JSON_MATRIX" | jq -e . > /dev/null 2>&1; then
                      echo "DEBUG: Input JSON_MATRIX was still considered VALID by jq -e just before the empty output. This is very strange."
                  else
                      echo "DEBUG: Input JSON_MATRIX was considered INVALID by jq -e. This is the likely cause of empty output."
                  fi
                  if [ "$JSON_MATRIX" == "" ]; then
                      echo "DEBUG: JSON_MATRIX was an empty string. Trying to initialize with the current chunk."
                      TEMP_JSON_MATRIX=$(jq -cn --arg d "$domain" --arg c "$chunk_file_path" '[{domain:$d,chunk:$c}]')
                      if [ -n "$TEMP_JSON_MATRIX" ]; then
                          echo "DEBUG: Successfully initialized TEMP_JSON_MATRIX from empty string case."
                      else
                          echo "ERROR: Still failed to initialize TEMP_JSON_MATRIX even from empty string case."
                          continue
                      fi
                  else
                      continue 
                  fi
                fi
                JSON_MATRIX="$TEMP_JSON_MATRIX"
                CHUNK_COUNT_FOR_THIS_FILE=$((CHUNK_COUNT_FOR_THIS_FILE + 1))
                echo "DEBUG: Successfully added/updated for chunk '$chunk_file_path'. JSON_MATRIX (first 100 chars): $(echo "$JSON_MATRIX" | head -c 100)"
              done < <(find "chunks/$domain/" -name 'chunk_*' -type f -print)
              if [ "$CHUNK_COUNT_FOR_THIS_FILE" -eq 0 ]; then
                echo "WARNING: No chunk files were processed/added to matrix for '$file_path' in domain '$domain'. 'split' might have created no files, or 'jq' failed for all."
              else
                echo "INFO: Processed and added $CHUNK_COUNT_FOR_THIS_FILE chunk(s) to matrix for domain '$domain' from '$file_path'."
              fi
            done
          fi
          echo "-----------------------------------------------------"
          if ! echo "$JSON_MATRIX" | jq -e . > /dev/null 2>&1; then
            echo "ERROR: Final JSON_MATRIX is not valid JSON. Content (first 200 chars): $(echo "$JSON_MATRIX" | head -c 200)"
            echo "WARNING: Setting JSON_MATRIX to '[]' and total_chunks to 0 due to invalid JSON."
            JSON_MATRIX="[]"
            TOTAL_CHUNKS=0
          else
            TOTAL_CHUNKS=$(echo "$JSON_MATRIX" | jq 'length')
          fi
          echo "FINAL_BUILD_INFO: Final JSON_MATRIX content (first 200 chars): $(echo "$JSON_MATRIX" | head -c 200)"
          echo "FINAL_BUILD_INFO: Total chunks in matrix: $TOTAL_CHUNKS"
          echo "INFO: Writing the final matrix to 'full_matrix.json' file..."
          echo "$JSON_MATRIX" > full_matrix.json
          if [ -f full_matrix.json ]; then
            echo "VERIFY: 'full_matrix.json' created. Size: $(wc -c < full_matrix.json) bytes."
            echo "VERIFY: Content preview of 'full_matrix.json' (first 200 chars): $(head -c 200 full_matrix.json)"
            if jq -e . full_matrix.json > /dev/null 2>&1; then
              echo "VERIFY: 'full_matrix.json' contains valid JSON."
            else
              echo "ERROR: 'full_matrix.json' does NOT contain valid JSON! This will cause issues for 'fromJson' later."
            fi
          else
            echo "CRITICAL_ERROR: 'full_matrix.json' was NOT created after attempting to write JSON_MATRIX."
            echo "INFO: Forcing 'full_matrix.json' to be '[]' as a fallback to prevent tar error."
            echo "[]" > full_matrix.json
          fi
          echo "full_matrix=$JSON_MATRIX" >> $GITHUB_OUTPUT
          echo "total_chunks=$TOTAL_CHUNKS" >> $GITHUB_OUTPUT                 
     
      - name: Package All Chunks and Resolvers      
        id: package_chunks # This ID is referenced by outputs and later steps
        if: steps.build_full_matrix.outputs.total_chunks > 0
        shell: bash
        run: |
          # Define the base name for the artifact (without .tar.gz)
          BASE_ARTIFACT_NAME="all-chunks-package-${{ github.run_id }}"
          # Define the full tar filename
          PACKAGE_TAR_FILENAME="${BASE_ARTIFACT_NAME}.tar.gz"
          echo "INFO: Preparing package '$PACKAGE_TAR_FILENAME'..."
          # Ensure resolver files and full_matrix.json exist, create fallbacks if not
          if [ ! -f resolvers.txt ]; then echo "WARNING: resolvers.txt not found, creating default." >&2; echo '1.1.1.1' > resolvers.txt; fi
          if [ ! -f resolvers-trusted.txt ]; then echo "WARNING: resolvers-trusted.txt not found, creating empty." >&2; touch resolvers-trusted.txt; fi
          if [ ! -f full_matrix.json ]; then echo "WARNING: full_matrix.json not found, creating empty array JSON file." >&2; echo "[]" > full_matrix.json; fi
          
          # Create the tarball
          echo "INFO: Creating tarball: $PACKAGE_TAR_FILENAME"
          tar -czvf "$PACKAGE_TAR_FILENAME" chunks resolvers.txt resolvers-trusted.txt full_matrix.json
          
          if [ $? -eq 0 ]; then
            echo "INFO: '$PACKAGE_TAR_FILENAME' created successfully."
          else
            echo "ERROR: Failed to create tarball '$PACKAGE_TAR_FILENAME'."
            # Exit or handle error appropriately if tar fails
            exit 0 
          fi
          
          # Output the full .tar.gz filename for the 'path' in upload-artifact
          echo "package_tar_filename=$PACKAGE_TAR_FILENAME" >> $GITHUB_OUTPUT
          # Output the base artifact name (without .tar.gz) for the 'name' in upload-artifact and for job output
          echo "base_artifact_name=$BASE_ARTIFACT_NAME" >> $GITHUB_OUTPUT
      
      - name: Upload Full Chunks Package
        if: steps.build_full_matrix.outputs.total_chunks > 0 && steps.package_chunks.outcome == 'success'
        uses: actions/upload-artifact@v4
        with:
          # Use the base artifact name output from the 'package_chunks' step
          name: ${{ steps.package_chunks.outputs.base_artifact_name }}
          # Use the full tar filename output from the 'package_chunks' step
          path: ${{ steps.package_chunks.outputs.package_tar_filename }}
          retention-days: 1

  distribute_and_trigger_secondary:
    name: Distribute Work & Trigger Secondary
    needs: prepare_all_chunks_and_package
    if: needs.prepare_all_chunks_and_package.outputs.total_chunks_count > 0 # Only run if there are chunks
    runs-on: ubuntu-latest
    outputs:
      primary_matrix_json: ${{ steps.calculate_distribution.outputs.primary_matrix }}
      secondary_matrix_json: ${{ steps.calculate_distribution.outputs.secondary_matrix }}
      tertiary_matrix_json: ${{ steps.calculate_distribution.outputs.tertiary_matrix }}
      secondary_processing_triggered: ${{ steps.trigger_secondary.outcome == 'success' && steps.calculate_distribution.outputs.secondary_chunks_exist == 'true' }}
      tertiary_processing_triggered: ${{ steps.trigger_tertiary.outcome == 'success' && steps.calculate_distribution.outputs.tertiary_chunks_exist == 'true' }}
   
    steps:
      - name: Calculate Chunk Distribution for Accounts
        id: calculate_distribution
        shell: bash
        run: |
          ALL_CHUNKS_JSON='${{ needs.prepare_all_chunks_and_package.outputs.all_chunks_matrix_json }}'
          TOTAL_CHUNKS=${{ needs.prepare_all_chunks_and_package.outputs.total_chunks_count }}
          THRESHOLD=${{ env.DISTRIBUTION_THRESHOLD }}

          echo "Total chunks generated: $TOTAL_CHUNKS"
          echo "Distribution threshold: $THRESHOLD"

          if [ "$TOTAL_CHUNKS" -lt "$THRESHOLD" ]; then
            # --- STRATEGY 1: Below Threshold -> Assign all to Primary ---
            echo "Total chunks are below threshold. Assigning all work to Primary Account."
            PRIMARY_CHUNKS_JSON="$ALL_CHUNKS_JSON"
            SECONDARY_CHUNKS_JSON="[]"
            TERTIARY_CHUNKS_JSON="[]"
          else
            # --- STRATEGY 2: At or Above Threshold -> Apply Percentage Split ---
            echo "Total chunks are at or above threshold. Applying 30-30-40 percentage split."
            PRIMARY_PERCENT=${{ env.PRIMARY_PERCENTAGE }}
            SECONDARY_PERCENT=${{ env.SECONDARY_PERCENTAGE }}

            PRIMARY_CHUNK_COUNT=$(echo "($TOTAL_CHUNKS * $PRIMARY_PERCENT) / 100" | bc)
            SECONDARY_CHUNK_COUNT=$(echo "($TOTAL_CHUNKS * $SECONDARY_PERCENT) / 100" | bc)

            echo "Calculated counts: Primary=$PRIMARY_CHUNK_COUNT, Secondary=$SECONDARY_CHUNK_COUNT, Tertiary gets the rest."
            OFFSET_FOR_TERTIARY=$((PRIMARY_CHUNK_COUNT + SECONDARY_CHUNK_COUNT))

            PRIMARY_CHUNKS_JSON=$(echo "$ALL_CHUNKS_JSON" | jq -c --argjson limit "$PRIMARY_CHUNK_COUNT" '.[0:$limit]')
         
            SECONDARY_CHUNKS_JSON=$(echo "$ALL_CHUNKS_JSON" | jq -c --argjson offset "$PRIMARY_CHUNK_COUNT" --argjson limit "$SECONDARY_CHUNK_COUNT" '.[$offset : $offset+$limit]')
                   
          
            TERTIARY_CHUNKS_JSON=$(echo "$ALL_CHUNKS_JSON" | jq -c --argjson offset "$OFFSET_FOR_TERTIARY" '.[$offset:]')
 
          fi
          # Set outputs based on the chosen strategy
          echo "primary_matrix=$PRIMARY_CHUNKS_JSON" >> $GITHUB_OUTPUT
          
          if [ "$(echo "$SECONDARY_CHUNKS_JSON" | jq 'length')" -eq 0 ]; then
            echo "secondary_matrix=[]" >> $GITHUB_OUTPUT; echo "secondary_chunks_exist=false" >> $GITHUB_OUTPUT
          else
            echo "secondary_matrix=$SECONDARY_CHUNKS_JSON" >> $GITHUB_OUTPUT; echo "secondary_chunks_exist=true" >> $GITHUB_OUTPUT
          fi
          
          if [ "$(echo "$TERTIARY_CHUNKS_JSON" | jq 'length')" -eq 0 ]; then
            echo "tertiary_matrix=[]" >> $GITHUB_OUTPUT; echo "tertiary_chunks_exist=false" >> $GITHUB_OUTPUT
          else
            echo "tertiary_matrix=$TERTIARY_CHUNKS_JSON" >> $GITHUB_OUTPUT; echo "tertiary_chunks_exist=true" >> $GITHUB_OUTPUT
          fi
          echo "Distribution calculation complete."
     
      #====================================================================
      # ADD THE FIRST SLEEP STEP HERE
      # ====================================================================
      - name: Add delay before triggering Secondary worker
        if: steps.calculate_distribution.outputs.secondary_chunks_exist == 'true'
        run: |
          echo "Pausing for 15 seconds to space out worker triggers and avoid platform throttling..."
          sleep 15
      # ====================================================================

      
      - name: Prepare Trigger Payload For Secondary
        id: prepare_payload_secondary
        if: steps.calculate_distribution.outputs.secondary_chunks_exist == 'true'
        shell: bash
        run: |
          # Ensure the secondary_matrix output from the previous step is treated as a string
          # that already represents a JSON array. jq's --argjson will parse it.
          MATRIX_AS_STRING='${{ steps.calculate_distribution.outputs.secondary_matrix }}'
          # Validate that MATRIX_AS_STRING is actually valid JSON before --argjson
          if ! echo "${MATRIX_AS_STRING}" | jq -e . > /dev/null 2>&1; then
            echo "::error title=Invalid Secondary Matrix::The secondary_matrix output ('${MATRIX_AS_STRING}') is not valid JSON. Defaulting to empty array."
            MATRIX_AS_STRING="[]" # Fallback to an empty JSON array string
          fi
          # Construct the entire payload as a valid JSON string using jq
          # --argjson tells jq to parse the value of matrix_json as JSON, not treat it as a literal string
          JSON_PAYLOAD=$(jq -cn \
            --arg run_id "${{ github.run_id }}" \
            '{
              "primary_run_id": $run_id
            }')
             echo "Constructed JSON Payload: $JSON_PAYLOAD"
             echo "json_string=$JSON_PAYLOAD" >> "$GITHUB_OUTPUT"
           
          JSON_PAYLOAD=$(jq -cn \
            --arg server_url "${{ github.server_url }}" \
            --arg repo_owner "${{ github.repository_owner }}" \
            --arg repo_name "${{ github.event.repository.name }}" \
            --arg run_id "${{ github.run_id }}" \
            --arg artifact_name "${{ needs.prepare_all_chunks_and_package.outputs.chunk_package_artifact_name }}" \
            --arg matrix_as_a_string "${MATRIX_AS_STRING}" \
            '{
              "primary_github_server_url": $server_url,
              "primary_repo_owner": $repo_owner,
              "primary_repo_name": $repo_name,
              "primary_run_id": $run_id,
              "chunk_package_artifact_name": $artifact_name,
              "secondary_matrix_json": $matrix_as_a_string 
            }')
          
          echo "Constructed JSON Payload: $JSON_PAYLOAD"
          # Set the constructed JSON string as an output of this step
          echo "json_string=$JSON_PAYLOAD" >> $GITHUB_OUTPUT
      
      - name: DEBUG - Show Secondary Inputs String
        if: steps.calculate_distribution.outputs.secondary_chunks_exist == 'true'
        run: |
          echo "DEBUG: Inputs for SECONDARY account:"
          #echo "${{ steps.prepare_payload_secondary.outputs.json_string }}"
      
      - name: Trigger Secondary Account Workflow
        id: trigger_secondary 
        if: steps.calculate_distribution.outputs.secondary_chunks_exist == 'true'
        uses: benc-uk/workflow-dispatch@v1
        with:
          workflow: resolve.yml 
          repo: ${{ env.ACCOUNT2_REPO_OWNER }}/${{ env.ACCOUNT2_REPO_NAME }}
          token: ${{ secrets.PAT_FOR_SECONDARY_ACCOUNT_REPO }}
          inputs: ${{ steps.prepare_payload_secondary.outputs.json_string }}
          ref: main # Or your default branch name in Account 2

      #====================================================================
      # ADD THE FIRST SLEEP STEP HERE
      # ====================================================================
      - name: Add delay before triggering Tertiary Trigger worker
        if: steps.calculate_distribution.outputs.tertiary_chunks_exist == 'true'
        run: |
          echo "Pausing for 30 seconds to space out worker triggers and avoid platform throttling..."
          sleep 30
      # ====================================================================
      - name: Prepare Trigger Payload For Tertiary
        # This is a NEW step, a copy of the secondary payload step for Account 3.
        id: prepare_payload_tertiary
        if: steps.calculate_distribution.outputs.tertiary_chunks_exist == 'true'
        shell: bash
        run: |
          MATRIX_AS_STRING='${{ steps.calculate_distribution.outputs.tertiary_matrix }}'
          JSON_PAYLOAD=$(jq -cn \
            --arg run_id "${{ github.run_id }}" \
            '{
              "primary_run_id": $run_id
            }')
             echo "Constructed JSON Payload: $JSON_PAYLOAD"
             echo "json_string=$JSON_PAYLOAD" >> "$GITHUB_OUTPUT"
           
          JSON_PAYLOAD=$(jq -cn \
            --arg server_url "${{ github.server_url }}" \
            --arg repo_owner "${{ github.repository_owner }}" \
            --arg repo_name "${{ github.event.repository.name }}" \
            --arg run_id "${{ github.run_id }}" \
            --arg artifact_name "${{ needs.prepare_all_chunks_and_package.outputs.chunk_package_artifact_name }}" \
            --arg matrix_as_a_string "${MATRIX_AS_STRING}" \
            '{
              "primary_github_server_url": $server_url,
              "primary_repo_owner": $repo_owner,
              "primary_repo_name": $repo_name,
              "primary_run_id": $run_id,
              "chunk_package_artifact_name": $artifact_name,
              "tertiary_matrix_json": $matrix_as_a_string 
            }')
          
          echo "Constructed JSON Payload: $JSON_PAYLOAD"
          # Set the constructed JSON string as an output of this step
          echo "json_string=$JSON_PAYLOAD" >> $GITHUB_OUTPUT

      - name: DEBUG - Show Tertiary Inputs String
        if: steps.calculate_distribution.outputs.tertiary_chunks_exist == 'true'
        run: |
          echo "DEBUG: Inputs for TERTIARY account:"
          # echo "${{ steps.prepare_payload_tertiary.outputs.json_string }}"    

      - name: Trigger Tertiary Account Workflow
        # This is a NEW step, a copy of the secondary trigger step for Account 3.
        id: trigger_tertiary
        if: steps.calculate_distribution.outputs.tertiary_chunks_exist == 'true'
        uses: benc-uk/workflow-dispatch@v1
        with:
          workflow: resolve.yml
          repo: ${{ env.ACCOUNT3_REPO_OWNER }}/${{ env.ACCOUNT3_REPO_NAME }}
          token: ${{ secrets.PAT_FOR_TERTIARY_ACCOUNT_REPO }}
          inputs: ${{ steps.prepare_payload_tertiary.outputs.json_string }}
          ref: main


  resolve_primary_account_chunks:
    name: Resolve Primary Account Chunks (puredns + dsieve)
    needs: [prepare_all_chunks_and_package, distribute_and_trigger_secondary] # Depends on distribution logic
    if: needs.prepare_all_chunks_and_package.outputs.total_chunks_count > 0 && needs.distribute_and_trigger_secondary.outputs.primary_matrix_json != '[]'
    runs-on: ubuntu-latest
    container:
      image: ghcr.io/pcoder7/spider-puredns-actions:latest
      credentials:
        username: ${{ github.actor }}
        password: ${{ secrets.GHCR_TOKEN }}    
    strategy:
      fail-fast: false
      max-parallel: 20 
      matrix:
        pair: ${{ fromJson(needs.distribute_and_trigger_secondary.outputs.primary_matrix_json) }}
    steps:
      - name: Checkout repository (for results structure)
        uses: actions/checkout@v3

      - name: Download Full Chunks Package for Primary
        uses: actions/download-artifact@v4
        with:
          name: ${{ needs.prepare_all_chunks_and_package.outputs.chunk_package_artifact_name }}
          # Downloads to current directory

      - name: Extract Chunks for Primary
        shell: bash
        run: |
          PACKAGE_FILENAME="${{ needs.prepare_all_chunks_and_package.outputs.chunk_package_artifact_name }}.tar.gz"
          echo "Extracting $PACKAGE_FILENAME..."
          tar -xzvf "$PACKAGE_FILENAME"
          if [ ! -d "chunks" ]; then
             echo "ERROR: 'chunks/' not found after extraction!"
             exit 0
          fi
          echo "Extraction complete. 'chunks/' should be present."
          ls -R chunks/
      - name: Install dsieve
        run: |
          if command -v dsieve &> /dev/null; then
            echo "dsieve is already installed"
          else
            echo "Installing dsieve..."
            go install github.com/trickest/dsieve@latest
          fi
     
      - name: Install Tools
        run: |
          # Installing smap
          if ! command -v smap >/dev/null; then
            echo "Installing smap…"
            go install -v github.com/s0md3v/smap/cmd/smap@latest
          else
            echo "smap already in cache"
          fi    
          # Installing inscope
          if ! command -v inscope >/dev/null; then
            echo "Installing inscope…"
            go install -v github.com/tomnomnom/hacks/inscope@latest
          else
            echo "inscope already in cache"
          fi    
          
          if ! command -v anew >/dev/null; then
            echo "Installing anew…"
            go install -v github.com/tomnomnom/anew@latest
          else
            echo "anew already in cache"
          fi
          
          if ! command -v cut-cdn >/dev/null; then
            echo "Installing cut-cdn…"
            go install github.com/ImAyrix/cut-cdn@latest
          else
            echo "cut-cdn already in cache"
          fi     

          if ! command -v naabu >/dev/null; then
            echo "Installing naabu…"
            go install -v github.com/projectdiscovery/naabu/v2/cmd/naabu@latest
          else
            echo "naabu already in cache"
          fi

          pip3 install --no-cache-dir ipaddress
          
          echo "$HOME/go/bin" >> $GITHUB_PATH
          
  
      - name: Fetch wordlists
        shell: bash
        run: |
          if [ ! -f resolvers.txt ]; then
              wget -qO resolvers.txt \
              https://raw.githubusercontent.com/rix4uni/resolvers/refs/heads/main/resolvers.txt
              echo "resolvers.txt is downloaded"
          fi
          if [ ! -f resolvers-trusted.txt ]; then
              wget -qO resolvers-trusted.txt \
              https://raw.githubusercontent.com/and0x00/resolvers.txt/refs/heads/main/resolvers.txt
              echo "resolvers-trusted.txt is downloaded"
          fi          
     
      - name: Run puredns + pre-dsieve on subdomains + final filtering      
        id: run_puredns
        shell: bash
        run: |
          # The chunk file from the matrix is treated as the raw input
          RAW_RESULTS_FILE="${{ matrix.pair.chunk }}"
          PUREDNS_FILE="puredns_file.txt"
          RESOLVED_FILE="puredns_resolved.txt"
          TMP_CLEANMASSDNS=$(mktemp)
          MASSDNS="massdns.txt"
          MASSDNS_FILE="massdns_file.txt"
          
          if [ ! -s "$RAW_RESULTS_FILE" ]; then
            echo "INFO: Raw results file (chunk) is empty. Nothing to resolve."
            # Create empty files to prevent downstream errors
            touch "$PUREDNS_FILE" "$RESOLVED_FILE" "$MASSDNS_FILE"
            exit 0
          fi
          
          echo "INFO: Resolving subdomains using puredns..."
          # The puredns command itself remains unchanged as requested
          puredns resolve "$RAW_RESULTS_FILE" \
            -r resolvers.txt \
            --rate-limit 3000 \
            --skip-validation \
            --skip-wildcard-filter \
            --write "$PUREDNS_FILE" \
            --write-massdns "$MASSDNS" \
            --quiet >/dev/null 2>&1
                               
          echo "✅ PureDNS resolution complete. $(wc -l < "$PUREDNS_FILE") subdomains were successfully resolved."
          
          # Scoping is now applied *after* resolution
          cat "$PUREDNS_FILE" | inscope -s .scope > "$RESOLVED_FILE" || true
          
          # Also scope the massdns results to ensure A-records are for target domains
          awk 'NF { sub(/\.$/,"",$1); print }' "$MASSDNS" > "$TMP_CLEANMASSDNS"
          awk ' \
          {gsub(/\r$/,"");sub(/^[ \t]+/,"");sub(/[ \t]+$/,"")} \
          FNR==NR{if($0)patterns[++c]=$0;next} \
          !setup{regex="";for(i=1;i<=c;i++){regex=regex (i>1?"|":"") "("patterns[i]")"};if(regex=="")regex="^\b$";setup=1} \
          $2=="A" && $1~regex \
          ' .scope "$TMP_CLEANMASSDNS" | anew -q "$MASSDNS_FILE"
          
          echo "✅ MassDNS processing complete. $(wc -l < "$MASSDNS_FILE") in-scope massdns records found."
          
          # Cleanup intermediate files
          rm -f "$TMP_CLEANMASSDNS" "$MASSDNS" "$PUREDNS_FILE"
          
      - name: Map subdomains to ports with CDN filtering
        id: map_subdomains_cdn
        shell: bash
        run: |
          set -e
          trap '' SIGPIPE
          MASSDNS_FILE="massdns_file.txt"
          SMAP_FILE="smap.txt"
          OUTPUT="subdomain_ports.txt"
          PORTS="1,43,49,70,79,80,81,82,83,84,85,88,102,104,113,135,139,143,175,179,195,264,280,389,443,444,505,515,548,554,591,631,771,783,789,888,898,900,901,993,995,1026,1080,1099,1153,1177,1200,1214,1220,1234,1311,1314,1344,1433,1503,1515,1521,1599,1723,1830,1900,1962,2000,2001,2002,2030,2064,2081,2087,2181,2222,2306,2345,2404,2455,2525,2715,2761,2762,3000,3001,3002,3052,3128,3260,3299,3310,3388,3389,3460,3531,3689,4000,4157,4242,4369,4443,4444,4500,4567,4711,4786,4899,5000,5001,5007,5009,5010,5025,5060,5222,5269,5280,5427,5432,5672,5800,5801,5802,5900,5938,6000,6001,6103,6346,6544,6600,6668,6699,6969,7002,7007,7070,7100,7171,7415,7776,8000,8001,8002,8003,8004,8005,8006,8007,8008,8009,8010,8080,8081,8082,8083,8084,8085,8087,8088,8118,8126,8181,8291,8443,8880,8881,8882,8883,8884,8885,8886,8887,8888,9000,9001,9002,9003,9030,9050,9080,9090,9100,9530,9600,9633,9999,10000,10001,10005,10134,11112,12345,13013,13666,15000,18245,20000,20256,20547,21379,25001,25565,31337,35000,37777,44818,50000,54138,55000,55555,60129" 
          
          if [ ! -s "$MASSDNS_FILE" ]; then
            echo "INFO: massdns_file.txt is empty. Skipping port mapping."
            touch "$OUTPUT"
            exit 0
          fi
          
          # create per‑runner temp files
          TMP_IP2SUB=$(mktemp)
          TMP_IP_ONLY=$(mktemp)
          TMP_NONCDN=$(mktemp)
          TMP_CDN=$(mktemp)
          TMP_SMAP_NONCDN=$(mktemp)
          TMP_RUSTSCAN=$(mktemp)
          echo "▶ Cleaning & extracting A‑records from $MASSDNS_FILE…"
          # IP SUBDOMAIN
          awk '{ print $3, $1 }' "$MASSDNS_FILE" | sort -k1,1 > "$TMP_IP2SUB"
          echo "▶ Pulling unique IPs…"
          cut -d' ' -f1 "$TMP_IP2SUB" | sort -u > "$TMP_IP_ONLY"
          echo "▶ Filtering non‑CDN IPs with cut-cdn…"
          cat "$TMP_IP_ONLY" | cut-cdn -ua -t 50 -silent -o "$TMP_NONCDN"
          echo "✅ All done. TMP_IP_ONLY contains $(wc -l < "$TMP_IP_ONLY") IP"
          head -n5 "$TMP_IP_ONLY"
          echo "==================================================================="
          echo "✅ All done. TMP_NONCDN contains $(wc -l < "$TMP_NONCDN") IP"
          head -n5 "$TMP_NONCDN"
          echo "==================================================================="
          echo "▶ Deriving CDN IP list…"
          cat "$TMP_IP_ONLY" | anew -d "$TMP_NONCDN" > "$TMP_CDN"
          echo "✅ All done. TMP_CDN contains $(wc -l < "$TMP_CDN") IP"
          head -n5 "$TMP_CDN"
          echo "==================================================================="
          echo "▶ Running smap and rustscan on non‑CDN IPs…"
          
          naabu -l "$TMP_NONCDN" -passive -o "$TMP_SMAP_NONCDN" -no-color -silent || true
          # The rustscan command itself remains unchanged as requested
          rustscan -a "$TMP_NONCDN" -p "PORTS" --no-banner -t 5000 --tries 1 -u 50000 --scan-order "Random" -b 300 --greppable --accessible > "$TMP_RUSTSCAN" || true 
          
          cat "$TMP_RUSTSCAN" | awk -F ' -> ' '{ gsub(/[\[\]]/, "", $2); n = split($2, p, ","); for(i=1;i<=n;i++) print $1 ":" p[i] }' | anew -q "$TMP_SMAP_NONCDN" || true
          
          echo "✅ All done. TMP_SMAP_NONCDN contains $(wc -l < "$TMP_SMAP_NONCDN") IP" 
          echo "==================================================================="
          head -n50 "$TMP_SMAP_NONCDN"
          echo "==================================================================="
          
          echo "▶ Merging non‑CDN and CDN IP lists into $SMAP_FILE…"
          cat "$TMP_SMAP_NONCDN" "$TMP_CDN" | sort -u > "$SMAP_FILE"
          echo "✅ All done. SMAP_FILE contains $(wc -l < "$SMAP_FILE") IP" 
          echo "==================================================================="
          head -n50 "$SMAP_FILE"
          echo "==================================================================="
          echo "▶ Joining with $SMAP_FILE to produce subdomain:port or subdomain…"
          awk -F: '
            NF==2 { print $1, $2 }
            NF==1 { print $1, ""  }
          ' "$SMAP_FILE" \
            | sort -k1,1 \
            | join - "$TMP_IP2SUB" \
            | { 
              awk '
                NF >= 2 { 
                  if (NF == 3 && $2 ~ /^[0-9]+$/) { 
                    print $3 ":" $2 
                  } else { 
                    print $NF 
                  } 
                }
              '       
            } \
            > "$OUTPUT"
          echo "✅ Generated $OUTPUT (first 50 lines):"
          head -n50 "$OUTPUT"
          # cleanup
          rm -f "$TMP_IP2SUB" "$TMP_IP_ONLY" "$TMP_NONCDN" "$TMP_CDN" "$TMP_SMAP_NONCDN" "$TMP_RUSTSCAN"   
          
      - name: Sort Resolved Results into Root Domain Folders
        shell: bash
        run: |
          RESOLVED_FILE="puredns_resolved.txt"
          PORTS_INPUT_FILE="subdomain_ports.txt"
        
          echo "INFO: Sorting resolved results into root-domain specific files..."
          mkdir -p results 
          
          if [ ! -s "$RESOLVED_FILE" ] && [ ! -s "$PORTS_INPUT_FILE" ]; then
            echo "INFO: No resolved domains or port data to sort."
            exit 0
          fi
          
          # First, find which root domains are present in our *resolved* results
          dsieve -if "$RESOLVED_FILE" -f 2 | sort -u > temp_root_domains.txt
          
          echo "INFO: Found the following root domains to process:"
          cat temp_root_domains.txt
          
          # Now, loop through those root domains and extract their subdomains
          while read -r parent; do
            if [ -z "$parent" ]; then continue; fi
            mkdir -p "results/$parent"
            
            # Sort the simple puredns results
            outfile="results/$parent/puredns_results.txt"
            echo "  -> Filtering puredns results for '$parent' into '$outfile'"
            grep -E "(^|\\.)${parent//./\\.}(\$)" --color=never "$RESOLVED_FILE" | anew -q "$outfile" || true
            
            # Sort the enriched subdomain:port results
            ports_outfile="results/$parent/subdomain_ports.txt"
            echo "  -> Filtering port data for '$parent' into '$ports_outfile'"
            grep -E "(^|\\.)${parent//./\\.}(\:|\$)" --color=never "$PORTS_INPUT_FILE" | anew -q "$ports_outfile" || true
            
          done < temp_root_domains.txt
          rm temp_root_domains.txt

      - name: Compute SAFE_CHUNK (no slashes)
        run: |
          SAFE_CHUNK="${{ matrix.pair.chunk }}"
          SAFE_CHUNK="$(echo "$SAFE_CHUNK" | tr '/' '_')"
          echo "SAFE_CHUNK=$SAFE_CHUNK" >> $GITHUB_ENV

      - name: Upload Primary Account Results
        uses: actions/upload-artifact@v4
        with:
          name: recon-results-primary-${{ env.SAFE_CHUNK }}
          path: results/
          retention-days: 1

  # ====================================================================
  # JOB 4: Merge All Results (Identical to reference workflow)
  # ====================================================================
  merge_results:
    name: Merge All Distributed Results
    runs-on: ubuntu-latest
    needs: resolve_primary_account_chunks
    if: always()
    outputs:
      has_results: ${{ steps.check_artifacts.outputs.found }}
    steps:
      - name: Checkout repository
        uses: actions/checkout@v3
    
      - name: Download all result artifacts from all accounts
        uses: actions/download-artifact@v4
        with:
          pattern: 'recon-results-*'
          path: temp-aggregated-results
          merge-multiple: true

      - name: Check if artifacts were downloaded
        id: check_artifacts
        shell: bash
        run: |
          if [ -d "temp-aggregated-results" ] && [ -n "$(ls -A temp-aggregated-results)" ]; then
            echo "-> Artifacts found. Proceeding with merge."
            echo "found=true" >> $GITHUB_OUTPUT
          else
            echo "-> No artifacts found to merge. Skipping the rest of this job."
            echo "found=false" >> $GITHUB_OUTPUT
          fi
          
          
      - name: Consolidate all results into root domain folders
        id: consolidate
        shell: bash
        run: |
          set -e
          mkdir -p final_results
          if [ ! -d "temp-aggregated-results" ] || [ -z "$(ls -A temp-aggregated-results)" ]; then
            echo "::warning:: No result artifacts were found. Nothing to merge."
            echo "has_results=false" >> $GITHUB_OUTPUT
            exit 0
          fi
          echo "INFO: Aggregating all downloaded results..."
          for filepath in $(find temp-aggregated-results -type f -name "puredns_results.txt"); do
            parent_domain=$(basename "$(dirname "$filepath")")
            dest_file="final_results/$parent_domain/puredns_results.txt"
            mkdir -p "final_results/$parent_domain"
            cat "$filepath" >> "$dest_file"
          done
          echo "INFO: Aggregating subdomain port data..."
          for filepath in $(find temp-aggregated-results -type f -name "subdomain_ports.txt"); do
            parent_domain=$(basename "$(dirname "$filepath")")
            dest_file="final_results/$parent_domain/subdomain_ports.txt"
            mkdir -p "final_results/$parent_domain"
            cat "$filepath" >> "$dest_file"
          done
          echo "INFO: De-duplicating all aggregated files..."
          for final_file in $(find final_results -type f -name "*.txt"); do
              sort -u -o "$final_file" "$final_file"
          done
          if [ -z "$(ls -A final_results)" ]; then
            echo "::warning:: Result artifacts contained no valid data to merge."
            echo "has_results=false" >> $GITHUB_OUTPUT
            exit 0
          fi
          echo "has_results=true" >> $GITHUB_OUTPUT
          echo "✅ Successfully consolidated results from all accounts."
          ls -R final_results
          
      - name: Upload Final Consolidated Artifact
        if: steps.consolidate.outputs.has_results == 'true'
        uses: actions/upload-artifact@v4
        with:
          name: consolidated-recon-results
          path: final_results/
          retention-days: 1

  # ====================================================================
  # JOB 5: Commit All Results (NEW - Identical to reference workflow)
  # ====================================================================
  commit_all_results:
    name: Commit All Results
    needs: merge_results
    if: always() && needs.merge_results.outputs.has_results == 'true'
    runs-on: ubuntu-latest
    container:
      image: ghcr.io/pcoder7/spider-puredns-actions:latest
      credentials:
        username: ${{ github.actor }}
        password: ${{ secrets.GHCR_TOKEN }}
    steps:
      - name: Download the single consolidated results artifact
        uses: actions/download-artifact@v4
        with:
          name: consolidated-recon-results
          path: final_results

      - name: Organize and Push to store-recon
        shell: bash
        env:
          STORE_RECON_PAT: ${{ secrets.PAT_FOR_SECONDRY }}
          ACCOUNT2_USERNAME: ${{ secrets.ACCOUNT2_REPO_OWNER }}
          STORE: ${{secrets.STORE}}
          CORRELATION_ID: ${{ github.run_id }}
        run: |
          RESULTS_DIR="${GITHUB_WORKSPACE}/final_results"

          if [ ! -d "$RESULTS_DIR" ] || [ -z "$(ls -A "$RESULTS_DIR")" ]; then
            echo "::warning:: Results directory is empty or does not exist. Nothing to commit."
            exit 0
          fi
          
          echo "Cloning ${STORE} to commit results from secondary worker..."
          git config --global user.name "Secondary Worker Bot"
          git config --global user.email "actions-bot@users.noreply.github.com"
          
          TMP_DIR="$(mktemp -d)"
          if ! git clone "https://x-access-token:${STORE_RECON_PAT}@github.com/${ACCOUNT2_USERNAME}/${STORE}.git" "$TMP_DIR"; then
            echo "::error:: Failed to clone the repository. Aborting the commit process."
            exit 0
          fi
          cd "$TMP_DIR"
          
          # --- Reusable function to merge artifact data ---
          run_merge() {
            echo "Merging new secondary results into the repository..."
            for domain_dir in "${RESULTS_DIR}"/*; do
              if [ ! -d "$domain_dir" ]; then continue; fi
              domain_name=$(basename "$domain_dir")
              dest_repo_dir="results/$domain_name"
              mkdir -p "$dest_repo_dir"

              # SAFE MERGE 1 (Unchanged as requested)
              source_puredns_file="$domain_dir/puredns_results.txt"
              dest_all_subs_file="$dest_repo_dir/all_subdomains.txt"
              if [ -s "$source_puredns_file" ]; then
                <"$source_puredns_file" tr -d '\0' \
                  | grep '[[:alnum:]]' \
                  | sed -e 's/^[[:space:]]*//' -e 's/[[:space:]]*$//' \
                  | sed -r "s/\x1B\[([0-9]{1,2}(;[0-9]{1,2})?)?[m|K]//g" \
                  > "$source_puredns_file.tmp" && mv "$source_puredns_file.tmp" "$source_puredns_file"
                  
                temp_merged_file_1=$(mktemp)
                if [ -f "$dest_all_subs_file" ]; then cat "$source_puredns_file" "$dest_all_subs_file" | sort -u > "$temp_merged_file_1"; else sort -u "$source_puredns_file" > "$temp_merged_file_1"; fi
                mv "$temp_merged_file_1" "$dest_all_subs_file"
              fi
              
              # SAFE MERGE 2 (Unchanged as requested)
              source_ports_file="$domain_dir/subdomain_ports.txt"
              dest_puredns_file="$dest_repo_dir/puredns_result.txt"
              if [ -s "$source_ports_file" ]; then
                <"$source_ports_file" tr -d '\0' \
                  | grep '[[:alnum:]]' \
                  | sed -e 's/^[[:space:]]*//' -e 's/[[:space:]]*$//' \
                  | sed -r "s/\x1B\[([0-9]{1,2}(;[0-9]{1,2})?)?[m|K]//g" \
                  > "$source_ports_file.tmp" && mv "$source_ports_file.tmp" "$source_ports_file"

                temp_merged_file_2=$(mktemp)
                if [ -f "$dest_puredns_file" ]; then cat "$source_ports_file" "$dest_puredns_file" | sort -u > "$temp_merged_file_2"; else sort -u "$source_ports_file" > "$temp_merged_file_2"; fi
                mv "$temp_merged_file_2" "$dest_puredns_file"
              fi
            done
            
            # Stage all changes
            git add results/
          }

          # --- Main Logic ---

          # 1. Perform the initial merge and check for changes
          run_merge
          if git diff --cached --quiet; then
            echo "No new unique data to commit from secondary worker."
            exit 0
          fi
          
          # 2. Commit the changes locally
          echo "Committing changes locally..."
          git commit -m "feat: Add new assets from distributed permutation scan from Correlation ID: ${CORRELATION_ID}"
          
          # 3. Loop to sync and push the commit, with a robust retry mechanism
          MAX_ATTEMPTS=10
          for (( i=1; i<=MAX_ATTEMPTS; i++ )); do
            echo "[Attempt $i/$MAX_ATTEMPTS] Pushing changes..."
            
            # Optimistic push first
            if git push -v origin main; then
              echo "✅ Successfully pushed new secondary results to ${STORE} on attempt $i."
              exit 0 # Success!
            fi
            
            echo "::warning:: Push failed on attempt $i. Fetching latest changes from remote and re-applying local changes."
            
            # If push failed, fetch the latest state from the remote
            git fetch origin main
            if [ $? -ne 0 ]; then
                echo "::error:: Git fetch failed. Cannot safely retry."
                sleep $(( 5 * i ))
                continue # Try again
            fi
            
            # Reset local state to match remote, discarding the old local commit
            git reset --hard origin/main
            
            # Re-run the merge logic on top of the fresh, updated branch
            echo "Re-applying merge logic on top of the updated main branch..."
            run_merge
            
            # Check if there are still changes to commit after the re-merge
            if git diff --cached --quiet; then
              echo "No net new changes to commit after syncing with remote. Another run may have already pushed these results."
              exit 0
            fi
            
            # Re-commit the newly calculated changes
            echo "Re-committing changes for retry attempt..."
            git commit -m "feat: Add new assets from distributed permutation scan from Correlation ID: ${CORRELATION_ID} (retry)"

            sleep $(( 5 * i ))
          done

          echo "::error:: All $MAX_ATTEMPTS push attempts failed. The job will pass but the commit was NOT pushed."
          exit 0
  
  trigger_http_multiple_two:
    name: Trigger http multiple two
    needs: [commit_all_results]
    if: always() 
    runs-on: ubuntu-latest
    steps:
      - name: Trigger next workflow
        uses: benc-uk/workflow-dispatch@v1
        with:
          workflow: http_dispatch_two.yml
          repo: Pcoder7/http-multiple-two 
          token: ${{ secrets.PAT_TOKEN }}
          ref: main

          inputs: '{ "run_id": "${{ github.run_id }}", "results_repo": "${{ github.repository }}" }'
