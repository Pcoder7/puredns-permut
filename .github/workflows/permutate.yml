name: Distributed PureDNS - Dispatcher

on:
  workflow_dispatch:

permissions:
  contents: write   # For creating matrix.json, artifacts, and potentially committing aggregated results
  actions: write    # For triggering workflows in Account 2's repository

env:
  LINES_PER_CHUNK: 5000
  PRIMARY_ACCOUNT_MAX_PARALLEL: 20 # Max parallel jobs for THIS account's resolve job
  # Define Account 2 repo details here or get from secrets for more flexibility
  ACCOUNT2_REPO_OWNER: ${{ secrets.ACCOUNT2_REPO_OWNER || 'pushrockzz' }} # Fallback for testing
  ACCOUNT2_REPO_NAME: ${{ secrets.ACCOUNT2_REPO_NAME || 'puredns-resolve' }} # Fallback for testing

jobs:
  prepare_all_chunks_and_package:
    name: Prepare All Chunks & Package
    runs-on: ubuntu-latest
    container:
      image: ghcr.io/pcoder7/spider-puredns-actions:latest
      credentials:
        username: ${{ github.actor }}
        password: ${{ secrets.GHCR_TOKEN }}
    outputs:
      # Output 1: The full JSON matrix string of ALL generated chunks
      all_chunks_matrix_json: ${{ steps.build_full_matrix.outputs.full_matrix }}
      # Output 2: The total number of chunks generated
      total_chunks_count: ${{ steps.build_full_matrix.outputs.total_chunks }}
      # Output 3: The name of the artifact containing all chunks and resolvers
      chunk_package_artifact_name: "all-chunks-package-${{ github.run_id }}"
    steps:
      - name: Checkout repository
        uses: actions/checkout@v3

      - name: DEBUG - Verify Tools and Initial File System State
        # ... (Your existing verification step - keep it, it's good!) ...
        shell: bash
        run: |
          echo "DEBUG: Current directory: $(pwd)"
          echo "DEBUG: Listing all files in repository root and subdirectories:"
          find . -print
          echo "-----------------------------------------------------"
          echo "DEBUG: Specifically looking for subdomains.txt files:"
          find . -type f -name 'subdomains.txt' -print || echo "DEBUG: 'find' for subdomains.txt failed or found nothing."
          echo "-----------------------------------------------------"
          echo "DEBUG: Checking for 'jq' command..."
          if ! command -v jq &> /dev/null; then
            echo "CRITICAL_ERROR: 'jq' command not found. This is essential. Aborting."
            exit 1
          else
            echo "DEBUG: jq version: $(jq --version)"
          fi
          echo "-----------------------------------------------------"
          echo "DEBUG: Checking for 'split' command..."
          if ! command -v split &> /dev/null; then
            echo "CRITICAL_ERROR: 'split' command not found. This is essential. Aborting."
            exit 1
          else
            echo "DEBUG: split version information (if available):"
            split --version || echo "DEBUG: Could not get split version."
          fi
          echo "-----------------------------------------------------"


      - name: Build Full Matrix & Create Chunks
        id: build_full_matrix
        shell: bash
        run: |

          echo "INFO: Locating 'subdomains.txt' files..."
          # Using a temporary file for 'find' results for robustness with special filenames
          # Ensure 'find' uses -print0 and 'xargs' uses -0 if you switch from the temp file method later.
          find . -type f -name "subdomains.txt" -print0 > found_files.tmp
          
          declare -a files=() # Explicitly declare as an array
          while IFS= read -r -d $'\0' file_path_from_find; do
            # Standardize path by removing leading './' if present
            standardized_file_path=$(echo "$file_path_from_find" | sed 's|^\./||')
            files+=("$standardized_file_path")
          done < found_files.tmp
          rm found_files.tmp # Clean up temporary file
          
          if [ "${#files[@]}" -eq 0 ]; then
            echo "WARNING: No 'subdomains.txt' files found in the repository."
            echo "INFO: JSON_MATRIX will remain '[]'. No chunks will be generated."
            # No need to exit here, let it proceed to write an empty full_matrix.json
            # and set outputs accordingly. The 'if' condition on the next job will handle this.
          else
            echo "INFO: Found ${#files[@]} 'subdomains.txt' file(s):"
            printf "  => '%s'\n" "${files[@]}" # Print each found file for clarity
          
            for file_path in "${files[@]}"; do
              echo "-----------------------------------------------------"
              echo "INFO: Processing file: '$file_path'"
          
              domain_dir=$(dirname "$file_path")
              # Deduce domain name. If file is in root, use filename (without .txt) as domain.
              if [ "$domain_dir" == "." ]; then
                domain=$(basename "$file_path" .txt)
              else
                domain=$(basename "$domain_dir")
              fi
              echo "INFO: Deduced domain: '$domain'"
          
              # Check if the file actually exists and is not empty before processing
              if [ ! -f "$file_path" ]; then
                echo "ERROR: File '$file_path' reported by find, but not found now. Skipping."
                continue
              fi
              if [ ! -s "$file_path" ]; then
                echo "WARNING: File '$file_path' is empty. Skipping splitting for this file."
                continue
              fi
          
              echo "INFO: Creating chunk directory 'chunks/$domain' if it doesn't exist."
              mkdir -p "chunks/$domain" # Ensures the target directory for split exists
          
              output_chunk_prefix="chunks/$domain/chunk_"
              echo "INFO: Splitting '$file_path' into chunks in 'chunks/$domain/' with prefix '$output_chunk_prefix' (max $LINES_PER_CHUNK lines/chunk)"
              
              split -l "$LINES_PER_CHUNK" -a 3 --numeric-suffixes=1 "$file_path" "$output_chunk_prefix"
              split_exit_code=$?
              if [ $split_exit_code -ne 0 ]; then
                echo "ERROR: 'split' command failed with exit code $split_exit_code for file '$file_path'. Skipping chunk generation for this file."
                continue # Move to the next subdomains.txt file
              fi
              
              CHUNK_COUNT_FOR_THIS_FILE=0
              # Iterate over generated chunk files for the current domain
              # Using find is generally more robust for filenames than shell globs, especially with nullglob nuances.
              while IFS= read -r chunk_file_path; do
                if [ -z "$chunk_file_path" ]; then continue; fi # Skip empty lines if find produces any
          
                # Make sure jq is available. This check is also done at the beginning of the job,
                # but a defensive check here before a critical operation is not bad.
                if ! command -v jq &> /dev/null; then
                  echo "CRITICAL_ERROR: 'jq' command not found during chunk processing. Aborting this file's chunk addition."
                  break # Exit this inner while loop for the current subdomains.txt
                fi
                
                # Append this {domain,chunk} object to our JSON_MATRIX
                # Using a temporary variable for the new matrix to check jq's success
                TEMP_JSON_MATRIX=$(printf '%s' "$JSON_MATRIX" | jq -c --arg d "$domain" --arg c "$chunk_file_path" '. + [{domain:$d,chunk:$c}]')
                jq_exit_code=$?
          
                if [ $jq_exit_code -ne 0 ]; then
                  echo "ERROR: jq command failed (exit code $jq_exit_code) when trying to add chunk '$chunk_file_path' for domain '$domain'."
                  echo "ERROR: Current JSON_MATRIX was (first 100 chars): $(echo "$JSON_MATRIX" | head -c 100)"
                  echo "ERROR: Arguments to jq were: domain='$domain', chunk_file_path='$chunk_file_path'"
                  # Decide on error handling: skip this chunk, or skip all chunks for this file?
                  # For now, skipping this chunk and continuing with others from this file.
                  continue
                elif [ -z "$TEMP_JSON_MATRIX" ]; then
                  echo "ERROR: jq command produced empty output for chunk '$chunk_file_path'. This should not happen with '. + [...]'."
                  echo "ERROR: Current JSON_MATRIX was (first 100 chars): $(echo "$JSON_MATRIX" | head -c 100)"
                  continue # Skip this potentially corrupted addition
                else
                  JSON_MATRIX="$TEMP_JSON_MATRIX" # Update the main JSON_MATRIX
                  CHUNK_COUNT_FOR_THIS_FILE=$((CHUNK_COUNT_FOR_THIS_FILE + 1))
                  # echo "DEBUG: Added chunk '$chunk_file_path'. JSON_MATRIX (first 100 chars): $(echo "$JSON_MATRIX" | head -c 100)"
                fi
              done < <(find "chunks/$domain/" -name 'chunk_*' -type f -print) # Use find to get chunk files
          
              if [ "$CHUNK_COUNT_FOR_THIS_FILE" -eq 0 ]; then
                echo "WARNING: No chunk files were processed/added to matrix for '$file_path' in domain '$domain'. 'split' might have created no files, or 'jq' failed for all."
              else
                echo "INFO: Processed and added $CHUNK_COUNT_FOR_THIS_FILE chunk(s) to matrix for domain '$domain' from '$file_path'."
              fi
            done # End of for loop iterating over subdomains.txt files
          fi # End of if [ "${#files[@]}" -eq 0 ]
          
          echo "-----------------------------------------------------"
          # Calculate total chunks from the final JSON_MATRIX
          # Ensure JSON_MATRIX is valid before trying to parse length with jq or writing to file
          # This check is important if errors in the loop might have corrupted JSON_MATRIX
          if ! echo "$JSON_MATRIX" | jq -e . > /dev/null 2>&1; then
            echo "ERROR: Final JSON_MATRIX is not valid JSON. Content (first 200 chars): $(echo "$JSON_MATRIX" | head -c 200)"
            echo "WARNING: Setting JSON_MATRIX to '[]' and total_chunks to 0 due to invalid JSON."
            JSON_MATRIX="[]"
            TOTAL_CHUNKS=0
          else
            TOTAL_CHUNKS=$(echo "$JSON_MATRIX" | jq 'length')
          fi
          
          echo "FINAL_BUILD_INFO: Final JSON_MATRIX content (first 200 chars): $(echo "$JSON_MATRIX" | head -c 200)"
          echo "FINAL_BUILD_INFO: Total chunks in matrix: $TOTAL_CHUNKS"
          
          # --- CRITICAL ADDITION: Write JSON_MATRIX to full_matrix.json file ---
          echo "INFO: Writing the final matrix to 'full_matrix.json' file..."
          echo "$JSON_MATRIX" > full_matrix.json
          # ---------------------------------------------------------------------
          
          # Verify file creation and content (important for debugging the tar error)
          if [ -f full_matrix.json ]; then
            echo "VERIFY: 'full_matrix.json' created. Size: $(wc -c < full_matrix.json) bytes."
            echo "VERIFY: Content preview of 'full_matrix.json' (first 200 chars): $(head -c 200 full_matrix.json)"
            # Further validation that it's valid JSON
            if jq -e . full_matrix.json > /dev/null 2>&1; then
              echo "VERIFY: 'full_matrix.json' contains valid JSON."
            else
              echo "ERROR: 'full_matrix.json' does NOT contain valid JSON! This will cause issues for 'fromJson' later."
            fi
          else
            echo "CRITICAL_ERROR: 'full_matrix.json' was NOT created after attempting to write JSON_MATRIX."
            echo "INFO: Forcing 'full_matrix.json' to be '[]' as a fallback to prevent tar error."
            echo "[]" > full_matrix.json # Create a fallback empty JSON array file
          fi
          
          # Set step outputs for the workflow to use
          # The 'full_matrix' output is the JSON string itself, not the filename.
          echo "full_matrix=$JSON_MATRIX" >> $GITHUB_OUTPUT
          echo "total_chunks=$TOTAL_CHUNKS" >> $GITHUB_OUTPUT                    

      - name: Package All Chunks and Resolvers
        id: package_chunks
        if: steps.build_full_matrix.outputs.total_chunks > 0
        shell: bash
        run: |
          ARTIFACT_NAME="all-chunks-package-${{ github.run_id }}"
          echo "INFO: Preparing package '$ARTIFACT_NAME.tar.gz'..."
          # Ensure resolvers.txt and resolvers-trusted.txt are present or created if needed
          if [ ! -f resolvers.txt ]; then echo "WARNING: resolvers.txt not found, creating default." >&2; echo '1.1.1.1' > resolvers.txt; fi
          if [ ! -f resolvers-trusted.txt ]; then echo "WARNING: resolvers-trusted.txt not found, creating empty." >&2; touch resolvers-trusted.txt; fi
          
          tar -czvf "$ARTIFACT_NAME.tar.gz" chunks resolvers.txt resolvers-trusted.txt full_matrix.json
          # We also add full_matrix.json to the tarball so Account 2 can reconstruct its portion if needed,
          # or Account 1 could send indices instead of the full secondary_matrix_json.
          # For now, we'll still send secondary_matrix_json directly.
          echo "$ARTIFACT_NAME.tar.gz created."
          echo "package_filename=$ARTIFACT_NAME.tar.gz" >> $GITHUB_OUTPUT

      - name: Upload Full Chunks Package
        if: steps.build_full_matrix.outputs.total_chunks > 0
        uses: actions/upload-artifact@v4
        with:
          name: ${{ needs.prepare_all_chunks_and_package.outputs.chunk_package_artifact_name }} # Use dynamic name
          path: ${{ steps.package_chunks.outputs.package_filename }}
          retention-days: 2 # Keep it for a couple of days for Account 2 to pick up

  distribute_and_trigger_secondary:
    name: Distribute Work & Trigger Secondary
    needs: prepare_all_chunks_and_package
    if: needs.prepare_all_chunks_and_package.outputs.total_chunks_count > 0 # Only run if there are chunks
    runs-on: ubuntu-latest
    outputs:
      primary_matrix_json: ${{ steps.calculate_distribution.outputs.primary_matrix }}
      secondary_processing_triggered: ${{ steps.trigger_secondary.outcome == 'success' && steps.calculate_distribution.outputs.secondary_chunks_exist == 'true' }}
    steps:
      - name: Calculate Chunk Distribution for Accounts
        id: calculate_distribution
        shell: bash
        run: |
          ALL_CHUNKS_JSON='${{ needs.prepare_all_chunks_and_package.outputs.all_chunks_matrix_json }}'
          PRIMARY_MAX_PARALLEL=${{ env.PRIMARY_ACCOUNT_MAX_PARALLEL }}
          
          echo "Full matrix (first 200 chars): $(echo "$ALL_CHUNKS_JSON" | head -c 200)"
          echo "Primary account max parallel: $PRIMARY_MAX_PARALLEL"

          # Chunks for Account 1 (Primary)
          CHUNKS_FOR_PRIMARY=$(echo "$ALL_CHUNKS_JSON" | jq -c --argjson limit "$PRIMARY_MAX_PARALLEL" '.[0:$limit]')
          echo "primary_matrix=$CHUNKS_FOR_PRIMARY" >> $GITHUB_OUTPUT
          echo "Primary matrix (first 200 chars): $(echo "$CHUNKS_FOR_PRIMARY" | head -c 200)"

          # Chunks for Account 2 (Secondary)
          CHUNKS_FOR_SECONDARY=$(echo "$ALL_CHUNKS_JSON" | jq -c --argjson offset "$PRIMARY_MAX_PARALLEL" '.[$offset:]')
          SECONDARY_COUNT=$(echo "$CHUNKS_FOR_SECONDARY" | jq 'length')

          if [ "$SECONDARY_COUNT" -eq 0 ]; then
            echo "No chunks remaining for secondary account."
            echo "secondary_matrix=[]" >> $GITHUB_OUTPUT
            echo "secondary_chunks_exist=false" >> $GITHUB_OUTPUT
          else
            echo "Secondary matrix (first 200 chars): $(echo "$CHUNKS_FOR_SECONDARY" | head -c 200)"
            echo "secondary_matrix=$CHUNKS_FOR_SECONDARY" >> $GITHUB_OUTPUT
            echo "secondary_chunks_exist=true" >> $GITHUB_OUTPUT
          fi

      - name: Trigger Secondary Account Workflow
        id: trigger_secondary
        if: steps.calculate_distribution.outputs.secondary_chunks_exist == 'true'
        uses: benc-uk/workflow-dispatch@v1  # Popular choice for cross-repo workflow dispatch
        with:
          workflow: resolve.yml # Name of the YML file in Account 2's repo
          repo: ${{ env.ACCOUNT2_REPO_OWNER }}/${{ env.ACCOUNT2_REPO_NAME }}
          token: ${{ secrets.PAT_FOR_SECONDARY }} # PAT from Account 2, with repo & workflow scope
          # client-payload is data sent to the triggered workflow
          client-payload: >-
            {
              "primary_github_server_url": "${{ github.server_url }}",
              "primary_repo_owner": "${{ github.repository_owner }}",
              "primary_repo_name": "${{ github.event.repository.name }}",
              "primary_run_id": "${{ github.run_id }}",
              "chunk_package_artifact_name": "${{ needs.prepare_all_chunks_and_package.outputs.chunk_package_artifact_name }}",
              "secondary_matrix_json": ${{ steps.calculate_distribution.outputs.secondary_matrix }}
            }
        # Note on client-payload: If secondary_matrix_json is huge, this can fail.
        # A more robust method is to send indices or have Account 2 parse the full_matrix.json from the downloaded package.
        # For now, this direct approach is simpler to start with.

  resolve_primary_account_chunks:
    name: Resolve Primary Account Chunks
    needs: [prepare_all_chunks_and_package, distribute_and_trigger_secondary] # Depends on distribution logic
    if: needs.prepare_all_chunks_and_package.outputs.total_chunks_count > 0 && needs.distribute_and_trigger_secondary.outputs.primary_matrix_json != '[]'
    runs-on: ubuntu-latest
    container:
      image: ghcr.io/pcoder7/spider-puredns-actions:latest
      credentials:
        username: ${{ github.actor }}
        password: ${{ secrets.GHCR_TOKEN }}
    strategy:
      fail-fast: false
      max-parallel: 20 
      matrix:
        pair: ${{ fromJson(needs.distribute_and_trigger_secondary.outputs.primary_matrix_json) }}
    steps:
      - name: Checkout repository (for results structure, not chunks)
        uses: actions/checkout@v3

      - name: Download Full Chunks Package for Primary
        uses: actions/download-artifact@v4
        with:
          name: ${{ needs.prepare_all_chunks_and_package.outputs.chunk_package_artifact_name }}
          # No path needed, downloads to current directory

      - name: Extract Chunks and Resolvers for Primary
        shell: bash
        run: |
          PACKAGE_FILENAME="${{ needs.prepare_all_chunks_and_package.outputs.chunk_package_artifact_name }}.tar.gz"
          echo "Extracting $PACKAGE_FILENAME..."
          tar -xzvf "$PACKAGE_FILENAME"
          if [ ! -d "chunks" ] || [ ! -f "resolvers.txt" ]; then
             echo "ERROR: chunks directory or resolvers.txt not found after extraction!"
             exit 1
          fi
          echo "Extraction complete. 'chunks/' and 'resolvers.txt' should be present."
          ls -R chunks/ # Verify
          ls resolvers.txt resolvers-trusted.txt

      - name: Resolve chunk via PureDNS (Primary Account)
        id: run_resolve_primary
        shell: bash
        run: |
          DOMAIN=${{ matrix.pair.domain }}
          CHUNK_FILE_PATH=${{ matrix.pair.chunk }} # This is the path like "chunks/domainA/chunk_001"

          echo "PRIMARY ACCOUNT: Resolving chunk '$CHUNK_FILE_PATH' for domain '$DOMAIN'..."
          if [ ! -f "$CHUNK_FILE_PATH" ]; then
            echo "ERROR: Chunk file '$CHUNK_FILE_PATH' not found locally!"
            exit 1
          fi

          OUT_DIR_PRIMARY="results_primary/$DOMAIN"
          mkdir -p "$OUT_DIR_PRIMARY"
          CHUNK_BASENAME=$(basename "$CHUNK_FILE_PATH")
          OUT_FILE="$OUT_DIR_PRIMARY/resolved_${CHUNK_BASENAME}"
          WILDCARD_FILE="$OUT_DIR_PRIMARY/wildcard_${CHUNK_BASENAME}"

          puredns resolve "$CHUNK_FILE_PATH" \
            -r resolvers.txt \
            --resolvers-trusted resolvers-trusted.txt \
            --write "$OUT_FILE" \
            --write-wildcards "$WILDCARD_FILE" \
            --wildcard-batch 100000 --wildcard-tests 250
          
          echo "CHUNK_BASENAME_PRIMARY=$CHUNK_BASENAME" >> $GITHUB_ENV

      - name: Upload Primary Account Resolved Results
        uses: actions/upload-artifact@v4
        with:
          name: primary_resolved_${{ matrix.pair.domain }}_${{ env.CHUNK_BASENAME_PRIMARY }}
          path: results_primary/${{ matrix.pair.domain }}/resolved_${{ env.CHUNK_BASENAME_PRIMARY }}
          retention-days: 7 # Keep results longer

  # Optional: Job to aggregate results IF NOT handled by Account 2
  # aggregate_and_commit_all_results:
  #   name: Aggregate All Results and Commit to Account 2 Repo
  #   needs: [resolve_primary_account_chunks, distribute_and_trigger_secondary]
  #   # This job should only run if secondary processing was triggered and primary also ran.
  #   # Determining when Account 2 is "done" is complex without external signals or S3 polling.
  #   # For simplicity, this example assumes you might run it manually or after a delay.
  #   # Or Account 2 could trigger THIS job back once it's done.
  #   if: always() && needs.distribute_and_trigger_secondary.outputs.secondary_processing_triggered == 'true'
  #   runs-on: ubuntu-latest
  #   steps:
  #     - name: Wait for secondary processing (heuristic)
  #       run: |
  #         echo "Waiting for a hypothetical period for Account 2 to finish..."
  #         sleep 300 # 5 minutes - VERY ROUGH, NOT RELIABLE

      # - name: Checkout Account 2 Repository
      #   uses: actions/checkout@v3
      #   with:
      #     repository: ${{ env.ACCOUNT2_REPO_OWNER }}/${{ env.ACCOUNT2_REPO_NAME }}
      #     token: ${{ secrets.PAT_FOR_ACCOUNT2_REPO_WRITE }} # PAT with write access to Account 2's repo
      #     path: repo_account2

      # - name: Download Primary Results Artifacts
      #   uses: actions/download-artifact@v4
      #   with:
      #     # This would need to download ALL primary_resolved_* artifacts.
      #     # download-artifact v4 supports pattern matching for names.
      #     name: primary_resolved_* 
      #     path: repo_account2/results_from_primary # Download to a specific subdir

      # - name: Download Secondary Results Artifacts (from Account 2)
      #   # This is the most complex part: how to get Account 2's artifacts here?
      #   # Option 1: Account 2 uploads to S3, this job downloads from S3.
      #   # Option 2: Account 2 triggers a workflow in THIS repo (Account 1) to signal completion and provide artifact details.
      #   # Option 3: Use GitHub API with Account 2's PAT to find and download its artifacts (needs Account 2 PAT here).
      #   run: echo "SIMULATING: Downloading Account 2's results..."

      # - name: Combine and Commit Results
      #   working-directory: ./repo_account2
      #   run: |
      #     # Logic to combine results from ./results_from_primary and (simulated) Account 2 results
      #     # into a final 'results/' directory.
      #     mkdir -p results
      #     # mv ./results_from_primary/*/* ./results/
      #     # mv (path_to_account2_results)/* ./results/
            
      #     git config --global user.name "GitHub Actions Aggregator"
      #     git config --global user.email "actions-aggregator@github.com"
      #     if [ -d "results" ] && [ "$(ls -A results)" ]; then
      #       git add results/
      #       if ! git diff --staged --quiet; then
      #         git commit -m "Aggregate PureDNS results from distributed run ${{ github.run_id }}"
      #         git push
      #       else
      #         echo "No new results to commit."
      #       fi
      #     else
      #       echo "Results directory is empty. Nothing to commit."
      #     fi
