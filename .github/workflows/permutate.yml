on:
  workflow_dispatch:

permissions:
  contents: write   # For creating matrix.json, artifacts, and potentially committing aggregated results
  actions: write    # For triggering workflows in Account 2's repository

env:
  # --- Chunking and Distribution Configuration ---
  LINES_PER_CHUNK: 200000 # Unchanged from original
  DISTRIBUTION_THRESHOLD: 21    # NEW: The minimum total chunks to trigger 3-way distribution
  PRIMARY_PERCENTAGE: 45        # NEW: 30% for the primary account
  SECONDARY_PERCENTAGE: 40      # NEW: 30% for the secondary account

  # Define Account 2 repo details here or get from secrets for more flexibility
  # --- Account 2 Details ---
  ACCOUNT2_REPO_OWNER: ${{ secrets.ACCOUNT2_REPO_OWNER || 'pushrockzz' }} # Unchanged from original
  ACCOUNT2_REPO_NAME: ${{ secrets.ACCOUNT2_REPO_NAME || 'puredns-resolve' }} # Unchanged from original

  # --- Account 3 Details ---
  ACCOUNT3_REPO_OWNER: ${{ secrets.ACCOUNT3_REPO_OWNER || 'Sari2705' }}      # NEW
  ACCOUNT3_REPO_NAME: ${{ secrets.ACCOUNT3_REPO_NAME || 'puredns-resolve' }} # NEW

jobs:
  prepare_all_chunks_and_package:
    name: Prepare All Chunks & Package
    runs-on: ubuntu-latest
    container:
      image: ghcr.io/pcoder7/spider-puredns-actions:latest
      credentials:
        username: ${{ github.actor }}
        password: ${{ secrets.GHCR_TOKEN }}
    outputs:
      # Output 1: The full JSON matrix string of ALL generated chunks
      all_chunks_matrix_json: ${{ steps.build_full_matrix.outputs.full_matrix }}
      # Output 2: The total number of chunks generated
      total_chunks_count: ${{ steps.build_full_matrix.outputs.total_chunks }}
      # Output 3: The name of the artifact containing all chunks and resolvers
      chunk_package_artifact_name: "all-chunks-package-${{ github.run_id }}"
    steps:
      - name: Checkout repository
        uses: actions/checkout@v3
      

      - name: Process, Generate, and Filter Permutations with Dynamic Wordlists
        env:
          MASTER_INPUT_LIST: "all-subdomains.txt" # More flexible way to set the input file
        shell: bash
        run: |
          # --- Setup, Input Files, and Tool Installation ---
          export TMPDIR="$RUNNER_TEMP"
          set -e # Exit immediately if a command exits with a non-zero status.

          echo "INFO: Verifying input file..."
          [ -f "$MASTER_INPUT_LIST" ] || { echo "CRITICAL: Missing input file '$MASTER_INPUT_LIST'."; exit 1; }



          echo "-----------------------------------------------------"

          # --- PRE-PROCESSING: Extract Unique Root Domains ---
          echo "PRE-PROCESSING: Extracting unique root domains from '$MASTER_INPUT_LIST'..."
          dsieve -if "$MASTER_INPUT_LIST" -f 2 | sort -u > unique_root_domains.txt
          ROOT_DOMAIN_COUNT=$(wc -l < unique_root_domains.txt)
          echo "INFO: Found $ROOT_DOMAIN_COUNT unique root domains to process."
          if [ "$ROOT_DOMAIN_COUNT" -eq 0 ]; then
            echo "WARNING: No root domains found. Exiting."
            touch subdomains.txt # Ensure file exists for subsequent steps
            exit 0
          fi
          echo "-----------------------------------------------------"



          # This file will hold all generated permutations before unique filtering
          > all_permutations_temp.txt

          # --- MAIN PROCESSING LOOP: Iterate over each root domain ---
          while read -r ROOT_DOMAIN; do
            if [ -z "$ROOT_DOMAIN" ]; then continue; fi
            echo "====================================================="
            echo "PROCESSING ROOT DOMAIN: $ROOT_DOMAIN"
            echo "====================================================="

            # Step 1: Create a temporary base list for the current root domain
            TEMP_BASE_LIST="${TMPDIR}/${ROOT_DOMAIN}-base.txt"
            echo "INFO: Creating temporary base list for '$ROOT_DOMAIN'..."
            grep -E "(^|\\.)${ROOT_DOMAIN//./\\.}(\$)" "$MASTER_INPUT_LIST" > "$TEMP_BASE_LIST"

            if [ ! -s "$TEMP_BASE_LIST" ]; then
                echo "WARNING: No subdomains found for '$ROOT_DOMAIN' in the master list. Skipping."
                continue
            fi

            # Step 2: Generate a dynamic, domain-specific wordlist
            DYNAMIC_WORDLIST="${TMPDIR}/${ROOT_DOMAIN}-wordlist.txt"
            echo "INFO: Generating dynamic wordlist for '$ROOT_DOMAIN'..."
            # This pipeline is efficient and correct
            dsieve -if "$TEMP_BASE_LIST" -f 3 -top 300 | awk -F '[-.]' '{for(i=1;i<=NF;i++) print $i}' | sort -u | grep -vE '^[0-9]+$|^.$' > "$DYNAMIC_WORDLIST"

            DYNAMIC_WORD_COUNT=$(wc -l < "$DYNAMIC_WORDLIST")
            if [ "$DYNAMIC_WORD_COUNT" -eq 0 ]; then
                echo "WARNING: Could not generate a dynamic wordlist for '$ROOT_DOMAIN'. Skipping permutation tools."
                continue
            fi
            echo "INFO: Generated dynamic wordlist with $DYNAMIC_WORD_COUNT words."

            # Step 3: Run permutation tools efficiently, once per tool per domain

                # Stage 3a: Gotator (Baseline for this root domain)
                echo "  -> Running gotator with dynamic wordlist..."
                # THE FIX: Build the command string first to handle quotes correctly.
                # We use 'printf' with '%q' to safely quote the wordlist path.
                SAFELY_QUOTED_WORDLIST=$(printf '%q' "$DYNAMIC_WORDLIST")
                GOTATOR_CMD="gotator -sub {} -depth 1 -adv -perm '"$DYNAMIC_WORDLIST"' -mindup -silent | anew -q {}-gotator.txt"
                
                chrunk -p 20 -i "$TEMP_BASE_LIST" -c 20 -prefix "${ROOT_DOMAIN}-gotator" -cmd "$GOTATOR_CMD"
                
                GOTATOR_RESULTS_TMP="${TMPDIR}/${ROOT_DOMAIN}-gotator-results.txt"
                cat "$TMPDIR"/chrunk-data/${ROOT_DOMAIN}-gotator-*.txt 2>/dev/null > "$GOTATOR_RESULTS_TMP"
                cat "$GOTATOR_RESULTS_TMP" | anew -q raw_permutations.txt
                wc -l "$GOTATOR_RESULTS_TMP"
                GOTATOR_COUNT=$(wc -l < "$GOTATOR_RESULTS_TMP")
                echo "    - Gotator generated $GOTATOR_COUNT initial permutations for $ROOT_DOMAIN."

                # Stage 3b: Dnsgen
                echo "  -> Running dnsgen with dynamic wordlist..."
                DNSGEN_CMD="dnsgen -w $SAFELY_QUOTED_WORDLIST {} > {}-dnsgen.txt"

                chrunk -p 20 -i "$TEMP_BASE_LIST" -c 20 -prefix "${ROOT_DOMAIN}-dnsgen" -cmd "$DNSGEN_CMD"
                
                DNSGEN_RESULTS_TMP="${TMPDIR}/${ROOT_DOMAIN}-dnsgen-results.txt"
                cat "$TMPDIR"/chrunk-data/${ROOT_DOMAIN}-dnsgen-*.txt 2>/dev/null > "$DNSGEN_RESULTS_TMP"
                DNSGEN_UNIQUE_ADDITIONS=$(cat "$DNSGEN_RESULTS_TMP" | anew -d raw_permutations.txt | wc -l)
                echo "    - Dnsgen found $DNSGEN_UNIQUE_ADDITIONS new unique subdomains."
                cat "$DNSGEN_RESULTS_TMP" | anew -q raw_permutations.txt
                
                # Stage 3c: Alterx
                echo "  -> Running alterx with dynamic wordlist..."
               
                # 'printf' also works for the alterx template path
                SAFELY_QUOTED_TEMPLATE=$(printf '%q' "template.txt")
                ALTERX_CMD="alterx -l {} -enrich -p $SAFELY_QUOTED_TEMPLATE -pp 'word=$SAFELY_QUOTED_WORDLIST' > {}-alterx.txt"
                
                chrunk -p 20 -i "$TEMP_BASE_LIST" -c 20 -prefix "${ROOT_DOMAIN}-alterx" -cmd "$ALTERX_CMD"

                ALTERX_RESULTS_TMP="${TMPDIR}/${ROOT_DOMAIN}-alterx-results.txt"
                cat "$TMPDIR"/chrunk-data/${ROOT_DOMAIN}-alterx-*.txt 2>/dev/null > "$ALTERX_RESULTS_TMP"
                ALTERX_UNIQUE_ADDITIONS=$(cat "$ALTERX_RESULTS_TMP" | anew -d raw_permutations.txt | wc -l)
                echo "    - Alterx found $ALTERX_UNIQUE_ADDITIONS new unique subdomains."
                cat "$ALTERX_RESULTS_TMP" | anew -q raw_permutations.txt
            
            echo "INFO: Finished processing for $ROOT_DOMAIN."
            # Clean up chrunk's temporary output directories for this root domain
            rm -rf "$TMPDIR"/chrunk-data/${ROOT_DOMAIN}-*
          done < unique_root_domains.txt

          # Now, de-duplicate all results at once
          echo "INFO: De-duplicating all generated permutations..."
          cat all_permutations_temp.txt | anew -q > raw_permutations.txt
          rm all_permutations_temp.txt

          echo "====================================================="
          # --- FINAL FILTERING STAGE ---
          echo "FINAL FILTERING: Removing jargon and complex numeric/IP-like patterns..."
              
          RAW_COUNT=$(wc -l < raw_permutations.txt)
          echo "INFO: Starting with $RAW_COUNT raw permutations."

          if [ "$RAW_COUNT" -eq 0 ]; then
              echo "WARNING: No raw permutations were generated. Final subdomains.txt will be empty."
              touch subdomains.txt
          else
              grep -ivE -f filter.txt raw_permutations.txt > subdomains.txt
          fi

          FINAL_COUNT=$(wc -l < subdomains.txt)
          FILTERED_COUNT=$((RAW_COUNT - FINAL_COUNT))

          echo "INFO: Filtered out $FILTERED_COUNT subdomains."
          echo "FINAL: The cleaned 'subdomains.txt' now contains $FINAL_COUNT permutations."
          wc -l subdomains.txt
          
      - name: DEBUG - Verify Tools and Initial File System State
        shell: bash
        run: |
          echo "DEBUG: Current directory: $(pwd)"
          echo "DEBUG: Listing all files in repository root and subdirectories:"
          find . -print
          echo "-----------------------------------------------------"
          echo "DEBUG: Specifically looking for subdomains.txt files:"
          find . -type f -name 'subdomains.txt' -print || echo "DEBUG: 'find' for subdomains.txt failed or found nothing."
          echo "-----------------------------------------------------"
          echo "DEBUG: Checking for 'jq' command..."
          if ! command -v jq &> /dev/null; then
            echo "CRITICAL_ERROR: 'jq' command not found. This is essential. Aborting."
            exit 1
          else
            echo "DEBUG: jq version: $(jq --version)"
          fi
          echo "-----------------------------------------------------"
          echo "DEBUG: Checking for 'split' command..."
          if ! command -v split &> /dev/null; then
            echo "CRITICAL_ERROR: 'split' command not found. This is essential. Aborting."
            exit 1
          else
            echo "DEBUG: split version information (if available):"
            split --version || echo "DEBUG: Could not get split version."
          fi
          echo "-----------------------------------------------------"
      - name: Build Full Matrix & Create Chunks
        id: build_full_matrix
        shell: bash
        run: |
          # --- Start of Script ---
          JSON_MATRIX='[]' # Initialize as an empty JSON array string
          echo "INFO: Initializing JSON_MATRIX as: '$JSON_MATRIX'"
          echo "INFO: Locating 'subdomains.txt' files..."
          find . -type f -name "subdomains.txt" -print0 > found_files.tmp
          declare -a files=() # Explicitly declare as an array
          while IFS= read -r -d $'\0' file_path_from_find; do
            standardized_file_path=$(echo "$file_path_from_find" | sed 's|^\./||')
            files+=("$standardized_file_path")
          done < found_files.tmp
          rm found_files.tmp # Clean up temporary file
          if [ "${#files[@]}" -eq 0 ]; then
            echo "WARNING: No 'subdomains.txt' files found in the repository."
            echo "INFO: JSON_MATRIX will remain '[]'. No chunks will be generated."
          else
            echo "INFO: Found ${#files[@]} 'subdomains.txt' file(s):"
            printf "  => '%s'\n" "${files[@]}" # Print each found file for clarity
            for file_path in "${files[@]}"; do
              echo "-----------------------------------------------------"
              echo "INFO: Processing file: '$file_path'"
              domain_dir=$(dirname "$file_path")
              if [ "$domain_dir" == "." ]; then
                domain=$(basename "$file_path" .txt)
              else
                domain=$(basename "$domain_dir")
              fi
              echo "INFO: Deduced domain: '$domain'"
              if [ ! -f "$file_path" ]; then
                echo "ERROR: File '$file_path' reported by find, but not found now. Skipping."
                continue
              fi
              if [ ! -s "$file_path" ]; then
                echo "WARNING: File '$file_path' is empty. Skipping splitting for this file."
                continue
              fi
              echo "INFO: Creating chunk directory 'chunks/$domain' if it doesn't exist."
              mkdir -p "chunks/$domain"
              output_chunk_prefix="chunks/$domain/chunk_"
              echo "INFO: Splitting '$file_path' into chunks in 'chunks/$domain/' with prefix '$output_chunk_prefix' (max $LINES_PER_CHUNK lines/chunk)"
              
              split -l "$LINES_PER_CHUNK" -a 3 --numeric-suffixes=1 "$file_path" "$output_chunk_prefix"
              split_exit_code=$?
              if [ $split_exit_code -ne 0 ]; then
                echo "ERROR: 'split' command failed with exit code $split_exit_code for file '$file_path'. Skipping chunk generation for this file."
                continue
              fi
              
              CHUNK_COUNT_FOR_THIS_FILE=0
              while IFS= read -r chunk_file_path; do
                if [ -z "$chunk_file_path" ]; then continue; fi
                echo "DEBUG: Processing chunk_file_path: '$chunk_file_path' for domain '$domain'"
                echo "DEBUG: Current JSON_MATRIX before jq (first 100 chars): $(echo "$JSON_MATRIX" | head -c 100)"
                echo "DEBUG: Validating current JSON_MATRIX with jq -e:"
                if echo "$JSON_MATRIX" | jq -e . > /dev/null 2>&1; then
                  echo "DEBUG: Current JSON_MATRIX is VALID."
                else
                  echo "CRITICAL_ERROR_DETECTED: Current JSON_MATRIX is INVALID before processing '$chunk_file_path'."
                  echo "CRITICAL_ERROR_DETECTED: JSON_MATRIX content: $JSON_MATRIX"
                  if [ "$JSON_MATRIX" != "[]" ] && [ "$CHUNK_COUNT_FOR_THIS_FILE" -gt 0 ]; then
                      echo "ABORTING chunk addition for this file due to previously corrupted JSON_MATRIX."
                      break 
                  else
                      echo "WARNING: JSON_MATRIX was invalid but appeared to be at an initial state. Attempting to reset to []."
                      JSON_MATRIX="[]"
                  fi
                fi
                
                if ! command -v jq &> /dev/null; then
                  echo "CRITICAL_ERROR: 'jq' command not found during chunk processing. Aborting this file's chunk addition."
                  break 
                fi
                
                echo "DEBUG: jq command to be run: printf '%s' \"\$JSON_MATRIX\" | jq -c --arg d \"$domain\" --arg c \"$chunk_file_path\" '. + [{domain:\$d,chunk:\$c}]'"
                TEMP_JSON_MATRIX=$(printf '%s' "$JSON_MATRIX" | jq -c --arg d "$domain" --arg c "$chunk_file_path" '. + [{domain:$d,chunk:$c}]')
                jq_exit_code=$?
                if [ $jq_exit_code -ne 0 ]; then
                  echo "ERROR: jq command failed (exit code $jq_exit_code) when trying to add chunk '$chunk_file_path' for domain '$domain'."
                  echo "ERROR: Input JSON_MATRIX was (first 100 chars): $(echo "$JSON_MATRIX" | head -c 100)"
                  echo "ERROR: Arguments to jq were: domain='$domain', chunk_file_path='$chunk_file_path'"
                  continue
                elif [ -z "$TEMP_JSON_MATRIX" ]; then
                  echo "ERROR: jq command produced empty output for chunk '$chunk_file_path'. This should not happen with '. + [...]'."
                  echo "ERROR: Input JSON_MATRIX that led to empty output (first 200 chars): $(echo "$JSON_MATRIX" | head -c 200)"
                  echo "ERROR: Full input JSON_MATRIX that led to empty output: $JSON_MATRIX"
                  echo "ERROR: Arguments to jq were: domain='$domain', chunk_file_path='$chunk_file_path'"
                  echo "DEBUG: Re-validating the input JSON_MATRIX that caused empty output:"
                  if echo "$JSON_MATRIX" | jq -e . > /dev/null 2>&1; then
                      echo "DEBUG: Input JSON_MATRIX was still considered VALID by jq -e just before the empty output. This is very strange."
                  else
                      echo "DEBUG: Input JSON_MATRIX was considered INVALID by jq -e. This is the likely cause of empty output."
                  fi
                  if [ "$JSON_MATRIX" == "" ]; then
                      echo "DEBUG: JSON_MATRIX was an empty string. Trying to initialize with the current chunk."
                      TEMP_JSON_MATRIX=$(jq -cn --arg d "$domain" --arg c "$chunk_file_path" '[{domain:$d,chunk:$c}]')
                      if [ -n "$TEMP_JSON_MATRIX" ]; then
                          echo "DEBUG: Successfully initialized TEMP_JSON_MATRIX from empty string case."
                      else
                          echo "ERROR: Still failed to initialize TEMP_JSON_MATRIX even from empty string case."
                          continue
                      fi
                  else
                      continue 
                  fi
                fi
                JSON_MATRIX="$TEMP_JSON_MATRIX"
                CHUNK_COUNT_FOR_THIS_FILE=$((CHUNK_COUNT_FOR_THIS_FILE + 1))
                echo "DEBUG: Successfully added/updated for chunk '$chunk_file_path'. JSON_MATRIX (first 100 chars): $(echo "$JSON_MATRIX" | head -c 100)"
              done < <(find "chunks/$domain/" -name 'chunk_*' -type f -print)
              if [ "$CHUNK_COUNT_FOR_THIS_FILE" -eq 0 ]; then
                echo "WARNING: No chunk files were processed/added to matrix for '$file_path' in domain '$domain'. 'split' might have created no files, or 'jq' failed for all."
              else
                echo "INFO: Processed and added $CHUNK_COUNT_FOR_THIS_FILE chunk(s) to matrix for domain '$domain' from '$file_path'."
              fi
            done
          fi
          echo "-----------------------------------------------------"
          if ! echo "$JSON_MATRIX" | jq -e . > /dev/null 2>&1; then
            echo "ERROR: Final JSON_MATRIX is not valid JSON. Content (first 200 chars): $(echo "$JSON_MATRIX" | head -c 200)"
            echo "WARNING: Setting JSON_MATRIX to '[]' and total_chunks to 0 due to invalid JSON."
            JSON_MATRIX="[]"
            TOTAL_CHUNKS=0
          else
            TOTAL_CHUNKS=$(echo "$JSON_MATRIX" | jq 'length')
          fi
          echo "FINAL_BUILD_INFO: Final JSON_MATRIX content (first 200 chars): $(echo "$JSON_MATRIX" | head -c 200)"
          echo "FINAL_BUILD_INFO: Total chunks in matrix: $TOTAL_CHUNKS"
          echo "INFO: Writing the final matrix to 'full_matrix.json' file..."
          echo "$JSON_MATRIX" > full_matrix.json
          if [ -f full_matrix.json ]; then
            echo "VERIFY: 'full_matrix.json' created. Size: $(wc -c < full_matrix.json) bytes."
            echo "VERIFY: Content preview of 'full_matrix.json' (first 200 chars): $(head -c 200 full_matrix.json)"
            if jq -e . full_matrix.json > /dev/null 2>&1; then
              echo "VERIFY: 'full_matrix.json' contains valid JSON."
            else
              echo "ERROR: 'full_matrix.json' does NOT contain valid JSON! This will cause issues for 'fromJson' later."
            fi
          else
            echo "CRITICAL_ERROR: 'full_matrix.json' was NOT created after attempting to write JSON_MATRIX."
            echo "INFO: Forcing 'full_matrix.json' to be '[]' as a fallback to prevent tar error."
            echo "[]" > full_matrix.json
          fi
          echo "full_matrix=$JSON_MATRIX" >> $GITHUB_OUTPUT
          echo "total_chunks=$TOTAL_CHUNKS" >> $GITHUB_OUTPUT                 
     
      - name: Package All Chunks and Resolvers      
        id: package_chunks # This ID is referenced by outputs and later steps
        if: steps.build_full_matrix.outputs.total_chunks > 0
        shell: bash
        run: |
          # Define the base name for the artifact (without .tar.gz)
          BASE_ARTIFACT_NAME="all-chunks-package-${{ github.run_id }}"
          # Define the full tar filename
          PACKAGE_TAR_FILENAME="${BASE_ARTIFACT_NAME}.tar.gz"
          echo "INFO: Preparing package '$PACKAGE_TAR_FILENAME'..."
          # Ensure resolver files and full_matrix.json exist, create fallbacks if not
          if [ ! -f resolvers.txt ]; then echo "WARNING: resolvers.txt not found, creating default." >&2; echo '1.1.1.1' > resolvers.txt; fi
          if [ ! -f resolvers-trusted.txt ]; then echo "WARNING: resolvers-trusted.txt not found, creating empty." >&2; touch resolvers-trusted.txt; fi
          if [ ! -f full_matrix.json ]; then echo "WARNING: full_matrix.json not found, creating empty array JSON file." >&2; echo "[]" > full_matrix.json; fi
          
          # Create the tarball
          echo "INFO: Creating tarball: $PACKAGE_TAR_FILENAME"
          tar -czvf "$PACKAGE_TAR_FILENAME" chunks resolvers.txt resolvers-trusted.txt full_matrix.json
          
          if [ $? -eq 0 ]; then
            echo "INFO: '$PACKAGE_TAR_FILENAME' created successfully."
          else
            echo "ERROR: Failed to create tarball '$PACKAGE_TAR_FILENAME'."
            # Exit or handle error appropriately if tar fails
            exit 1 
          fi
          
          # Output the full .tar.gz filename for the 'path' in upload-artifact
          echo "package_tar_filename=$PACKAGE_TAR_FILENAME" >> $GITHUB_OUTPUT
          # Output the base artifact name (without .tar.gz) for the 'name' in upload-artifact and for job output
          echo "base_artifact_name=$BASE_ARTIFACT_NAME" >> $GITHUB_OUTPUT
      
      - name: Upload Full Chunks Package
        if: steps.build_full_matrix.outputs.total_chunks > 0 && steps.package_chunks.outcome == 'success'
        uses: actions/upload-artifact@v4
        with:
          # Use the base artifact name output from the 'package_chunks' step
          name: ${{ steps.package_chunks.outputs.base_artifact_name }}
          # Use the full tar filename output from the 'package_chunks' step
          path: ${{ steps.package_chunks.outputs.package_tar_filename }}
          retention-days: 2

  distribute_and_trigger_secondary:
    name: Distribute Work & Trigger Secondary
    needs: prepare_all_chunks_and_package
    if: needs.prepare_all_chunks_and_package.outputs.total_chunks_count > 0 # Only run if there are chunks
    runs-on: ubuntu-latest
    outputs:
      primary_matrix_json: ${{ steps.calculate_distribution.outputs.primary_matrix }}
      secondary_matrix_json: ${{ steps.calculate_distribution.outputs.secondary_matrix }}
      tertiary_matrix_json: ${{ steps.calculate_distribution.outputs.tertiary_matrix }}
      secondary_processing_triggered: ${{ steps.trigger_secondary.outcome == 'success' && steps.calculate_distribution.outputs.secondary_chunks_exist == 'true' }}
      tertiary_processing_triggered: ${{ steps.trigger_tertiary.outcome == 'success' && steps.calculate_distribution.outputs.tertiary_chunks_exist == 'true' }}
   
    steps:
      - name: Calculate Chunk Distribution for Accounts
        id: calculate_distribution
        shell: bash
        run: |
          ALL_CHUNKS_JSON='${{ needs.prepare_all_chunks_and_package.outputs.all_chunks_matrix_json }}'
          TOTAL_CHUNKS=${{ needs.prepare_all_chunks_and_package.outputs.total_chunks_count }}
          THRESHOLD=${{ env.DISTRIBUTION_THRESHOLD }}

          echo "Total chunks generated: $TOTAL_CHUNKS"
          echo "Distribution threshold: $THRESHOLD"

          if [ "$TOTAL_CHUNKS" -lt "$THRESHOLD" ]; then
            # --- STRATEGY 1: Below Threshold -> Assign all to Primary ---
            echo "Total chunks are below threshold. Assigning all work to Primary Account."
            PRIMARY_CHUNKS_JSON="$ALL_CHUNKS_JSON"
            SECONDARY_CHUNKS_JSON="[]"
            TERTIARY_CHUNKS_JSON="[]"
          else
            # --- STRATEGY 2: At or Above Threshold -> Apply Percentage Split ---
            echo "Total chunks are at or above threshold. Applying 30-30-40 percentage split."
            PRIMARY_PERCENT=${{ env.PRIMARY_PERCENTAGE }}
            SECONDARY_PERCENT=${{ env.SECONDARY_PERCENTAGE }}

            PRIMARY_CHUNK_COUNT=$(echo "($TOTAL_CHUNKS * $PRIMARY_PERCENT) / 100" | bc)
            SECONDARY_CHUNK_COUNT=$(echo "($TOTAL_CHUNKS * $SECONDARY_PERCENT) / 100" | bc)

            echo "Calculated counts: Primary=$PRIMARY_CHUNK_COUNT, Secondary=$SECONDARY_CHUNK_COUNT, Tertiary gets the rest."
            OFFSET_FOR_TERTIARY=$((PRIMARY_CHUNK_COUNT + SECONDARY_CHUNK_COUNT))

            PRIMARY_CHUNKS_JSON=$(echo "$ALL_CHUNKS_JSON" | jq -c --argjson limit "$PRIMARY_CHUNK_COUNT" '.[0:$limit]')
         
            SECONDARY_CHUNKS_JSON=$(echo "$ALL_CHUNKS_JSON" | jq -c --argjson offset "$PRIMARY_CHUNK_COUNT" --argjson limit "$SECONDARY_CHUNK_COUNT" '.[$offset : $offset+$limit]')
                   
          
            TERTIARY_CHUNKS_JSON=$(echo "$ALL_CHUNKS_JSON" | jq -c --argjson offset "$OFFSET_FOR_TERTIARY" '.[$offset:]')
 
          fi
          # Set outputs based on the chosen strategy
          echo "primary_matrix=$PRIMARY_CHUNKS_JSON" >> $GITHUB_OUTPUT
          
          if [ "$(echo "$SECONDARY_CHUNKS_JSON" | jq 'length')" -eq 0 ]; then
            echo "secondary_matrix=[]" >> $GITHUB_OUTPUT; echo "secondary_chunks_exist=false" >> $GITHUB_OUTPUT
          else
            echo "secondary_matrix=$SECONDARY_CHUNKS_JSON" >> $GITHUB_OUTPUT; echo "secondary_chunks_exist=true" >> $GITHUB_OUTPUT
          fi
          
          if [ "$(echo "$TERTIARY_CHUNKS_JSON" | jq 'length')" -eq 0 ]; then
            echo "tertiary_matrix=[]" >> $GITHUB_OUTPUT; echo "tertiary_chunks_exist=false" >> $GITHUB_OUTPUT
          else
            echo "tertiary_matrix=$TERTIARY_CHUNKS_JSON" >> $GITHUB_OUTPUT; echo "tertiary_chunks_exist=true" >> $GITHUB_OUTPUT
          fi
          echo "Distribution calculation complete."


      
      - name: Prepare Trigger Payload For Secondary
        id: prepare_payload_secondary
        if: steps.calculate_distribution.outputs.secondary_chunks_exist == 'true'
        shell: bash
        run: |
          # Ensure the secondary_matrix output from the previous step is treated as a string
          # that already represents a JSON array. jq's --argjson will parse it.
          MATRIX_AS_STRING='${{ steps.calculate_distribution.outputs.secondary_matrix }}'
          # Validate that MATRIX_AS_STRING is actually valid JSON before --argjson
          if ! echo "${MATRIX_AS_STRING}" | jq -e . > /dev/null 2>&1; then
            echo "::error title=Invalid Secondary Matrix::The secondary_matrix output ('${MATRIX_AS_STRING}') is not valid JSON. Defaulting to empty array."
            MATRIX_AS_STRING="[]" # Fallback to an empty JSON array string
          fi
          # Construct the entire payload as a valid JSON string using jq
          # --argjson tells jq to parse the value of matrix_json as JSON, not treat it as a literal string
          JSON_PAYLOAD=$(jq -cn \
            --arg run_id "${{ github.run_id }}" \
            '{
              "primary_run_id": $run_id
            }')
             echo "Constructed JSON Payload: $JSON_PAYLOAD"
             echo "json_string=$JSON_PAYLOAD" >> "$GITHUB_OUTPUT"
           
          JSON_PAYLOAD=$(jq -cn \
            --arg server_url "${{ github.server_url }}" \
            --arg repo_owner "${{ github.repository_owner }}" \
            --arg repo_name "${{ github.event.repository.name }}" \
            --arg run_id "${{ github.run_id }}" \
            --arg artifact_name "${{ needs.prepare_all_chunks_and_package.outputs.chunk_package_artifact_name }}" \
            --arg matrix_as_a_string "${MATRIX_AS_STRING}" \
            '{
              "primary_github_server_url": $server_url,
              "primary_repo_owner": $repo_owner,
              "primary_repo_name": $repo_name,
              "primary_run_id": $run_id,
              "chunk_package_artifact_name": $artifact_name,
              "secondary_matrix_json": $matrix_as_a_string 
            }')
          
          echo "Constructed JSON Payload: $JSON_PAYLOAD"
          # Set the constructed JSON string as an output of this step
          echo "json_string=$JSON_PAYLOAD" >> $GITHUB_OUTPUT
      
      - name: DEBUG - Show Secondary Inputs String
        if: steps.calculate_distribution.outputs.secondary_chunks_exist == 'true'
        run: |
          echo "DEBUG: Inputs for SECONDARY account:"
          #echo "${{ steps.prepare_payload_secondary.outputs.json_string }}"
      
      - name: Trigger Secondary Account Workflow
        id: trigger_secondary 
        if: steps.calculate_distribution.outputs.secondary_chunks_exist == 'true'
        uses: benc-uk/workflow-dispatch@v1
        with:
          workflow: resolve.yml 
          repo: ${{ env.ACCOUNT2_REPO_OWNER }}/${{ env.ACCOUNT2_REPO_NAME }}
          token: ${{ secrets.PAT_FOR_SECONDARY_ACCOUNT_REPO }}
          inputs: ${{ steps.prepare_payload_secondary.outputs.json_string }}
          ref: main # Or your default branch name in Account 2


      - name: Prepare Trigger Payload For Tertiary
        # This is a NEW step, a copy of the secondary payload step for Account 3.
        id: prepare_payload_tertiary
        if: steps.calculate_distribution.outputs.tertiary_chunks_exist == 'true'
        shell: bash
        run: |
          MATRIX_AS_STRING='${{ steps.calculate_distribution.outputs.tertiary_matrix }}'
          JSON_PAYLOAD=$(jq -cn \
            --arg run_id "${{ github.run_id }}" \
            '{
              "primary_run_id": $run_id
            }')
             echo "Constructed JSON Payload: $JSON_PAYLOAD"
             echo "json_string=$JSON_PAYLOAD" >> "$GITHUB_OUTPUT"
           
          JSON_PAYLOAD=$(jq -cn \
            --arg server_url "${{ github.server_url }}" \
            --arg repo_owner "${{ github.repository_owner }}" \
            --arg repo_name "${{ github.event.repository.name }}" \
            --arg run_id "${{ github.run_id }}" \
            --arg artifact_name "${{ needs.prepare_all_chunks_and_package.outputs.chunk_package_artifact_name }}" \
            --arg matrix_as_a_string "${MATRIX_AS_STRING}" \
            '{
              "primary_github_server_url": $server_url,
              "primary_repo_owner": $repo_owner,
              "primary_repo_name": $repo_name,
              "primary_run_id": $run_id,
              "chunk_package_artifact_name": $artifact_name,
              "secondary_matrix_json": $matrix_as_a_string 
            }')
          
          echo "Constructed JSON Payload: $JSON_PAYLOAD"
          # Set the constructed JSON string as an output of this step
          echo "json_string=$JSON_PAYLOAD" >> $GITHUB_OUTPUT

      - name: DEBUG - Show Tertiary Inputs String
        if: steps.calculate_distribution.outputs.tertiary_chunks_exist == 'true'
        run: |
          echo "DEBUG: Inputs for TERTIARY account:"
          # echo "${{ steps.prepare_payload_tertiary.outputs.json_string }}"    

      - name: Trigger Tertiary Account Workflow
        # This is a NEW step, a copy of the secondary trigger step for Account 3.
        id: trigger_tertiary
        if: steps.calculate_distribution.outputs.tertiary_chunks_exist == 'true'
        uses: benc-uk/workflow-dispatch@v1
        with:
          workflow: resolve.yml
          repo: ${{ env.ACCOUNT3_REPO_OWNER }}/${{ env.ACCOUNT3_REPO_NAME }}
          token: ${{ secrets.PAT_FOR_TERTIARY_ACCOUNT_REPO }}
          inputs: ${{ steps.prepare_payload_tertiary.outputs.json_string }}
          ref: main


  resolve_primary_account_chunks:
    name: Resolve Primary Account Chunks (puredns + dsieve)
    needs: [prepare_all_chunks_and_package, distribute_and_trigger_secondary] # Depends on distribution logic
    if: needs.prepare_all_chunks_and_package.outputs.total_chunks_count > 0 && needs.distribute_and_trigger_secondary.outputs.primary_matrix_json != '[]'
    runs-on: ubuntu-latest
    container:
      image: ghcr.io/pcoder7/spider-puredns-actions:latest
      credentials:
        username: ${{ github.actor }}
        password: ${{ secrets.GHCR_TOKEN }}    
    strategy:
      fail-fast: false
      max-parallel: 20 
      matrix:
        pair: ${{ fromJson(needs.distribute_and_trigger_secondary.outputs.primary_matrix_json) }}
    steps:
      - name: Checkout repository (for results structure)
        uses: actions/checkout@v3

      - name: Download Full Chunks Package for Primary
        uses: actions/download-artifact@v4
        with:
          name: ${{ needs.prepare_all_chunks_and_package.outputs.chunk_package_artifact_name }}
          # Downloads to current directory

      - name: Extract Chunks for Primary
        shell: bash
        run: |
          PACKAGE_FILENAME="${{ needs.prepare_all_chunks_and_package.outputs.chunk_package_artifact_name }}.tar.gz"
          echo "Extracting $PACKAGE_FILENAME..."
          tar -xzvf "$PACKAGE_FILENAME"
          if [ ! -d "chunks" ]; then
             echo "ERROR: 'chunks/' not found after extraction!"
             exit 1
          fi
          echo "Extraction complete. 'chunks/' should be present."
          ls -R chunks/
      - name: Install dsieve
        run: |
          if command -v dsieve &> /dev/null; then
            echo "dsieve is already installed"
          else
            echo "Installing dsieve..."
            go install github.com/trickest/dsieve@latest
          fi
      
      - name: Fetch wordlists
        shell: bash
        run: |
          if [ ! -f resolvers.txt ]; then
              wget -qO resolvers.txt \
              https://raw.githubusercontent.com/rix4uni/resolvers/refs/heads/main/resolvers.txt
              echo "resolvers.txt is downloaded"
          fi
          if [ ! -f resolvers-trusted.txt ]; then
              wget -qO resolvers-trusted.txt \
              https://raw.githubusercontent.com/and0x00/resolvers.txt/refs/heads/main/resolvers.txt
              echo "resolvers-trusted.txt is downloaded"
          fi          
      - name: Run puredns + pre-dsieve on subdomains + final filtering
        shell: bash
        run: |
          DOMAIN=${{ matrix.pair.domain }}
          CHUNK_FILE_PATH=${{ matrix.pair.chunk }}   # e.g. "chunks/example.com/chunk_001"
          echo "Processing domain '$DOMAIN' with chunk '$CHUNK_FILE_PATH'..."
          if [ ! -f "$CHUNK_FILE_PATH" ]; then
            echo "ERROR: Chunk file '$CHUNK_FILE_PATH' not found!"
            exit 1
          fi
          # ───────────────────────────────────────────────────────────
          # Step A: pre-dsieve on the raw subdomain chunk (no puredns yet)
          # ───────────────────────────────────────────────────────────
          echo "-> Generating parent_domains.txt from raw subdomains..."
          dsieve -if "$CHUNK_FILE_PATH" -f 2 | sort -u > parent_domains.txt
          echo "  Parents in this chunk: $(wc -l < parent_domains.txt)"
          head -n 5 parent_domains.txt || true
          # ───────────────────────────────────────────────────────────
          # Step B: Probe all subdomains in this chunk with puredns
          # ───────────────────────────────────────────────────────────
          PUREDNS_OUT="puredns_output.txt"
          echo "-> Running puredns against '$CHUNK_FILE_PATH'..."
          puredns resolve "$CHUNK_FILE_PATH" \
            -r resolvers.txt \
            --resolvers-trusted resolvers-trusted.txt \
            --write "$PUREDNS_OUT" \
            --write-wildcards "$WILDCARD_FILE" \
            --wildcard-batch 100000 --wildcard-tests 250
            
          puredns_exit=$?
          if [ $puredns_exit -ne 0 ]; then
            echo "ERROR: puredns returned exit code $puredns_exit. Aborting chunk."
            exit 1
          fi
          # Always create results/ so upload-artifact sees at least an empty folder
          OUTPUT_ROOT="results"
          mkdir -p "$OUTPUT_ROOT"
          if [ ! -s "$PUREDNS_OUT" ]; then
            echo "No live hostnames found in this chunk. Leaving results/ empty."
            exit 0
          fi
          echo "-> Sample of puredns_output.txt (first 5 lines):"
          head -n 5 "$PUREDNS_OUT" || true
          # ───────────────────────────────────────────────────────────
          # Step C+D: For each parent domain, filter matching hostnames
          # ───────────────────────────────────────────────────────────
          echo "DEBUG: Starting per-parent filtering with AWK (domain-only mode)"
          while read -r parent; do
            # Strip any carriage returns or trailing whitespace
            clean_parent=$(printf '%s' "$parent" | tr -d '\r' | xargs)
            if [ -z "$clean_parent" ]; then
              echo "DEBUG: Skipping empty parent line"
              continue
            fi
            echo "DEBUG: Processing parent => [[$clean_parent]]"
            mkdir -p "$OUTPUT_ROOT/$clean_parent"
            outfile="$OUTPUT_ROOT/$clean_parent/puredns_result.txt"
            echo "DEBUG: AWK filtering host==$clean_parent or subdomain of $clean_parent from $PUREDNS_OUT into $outfile"
            awk -v b="$clean_parent" '
            {
              host = $1
              if (host == b || host ~ ("\\." b "$")) {
                print $0
              }
            }
            ' "$PUREDNS_OUT" > "$outfile" 2>/tmp/awk_error.log
            awk_exit=$?
            if [ $awk_exit -ne 0 ]; then
              echo "ERROR: AWK exited with code $awk_exit for parent='$clean_parent'."
              echo "DEBUG: Contents of /tmp/awk_error.log:"
              cat /tmp/awk_error.log || true
            fi
            if [ -f "$outfile" ]; then
              lines_written=$(wc -l < "$outfile")
              echo "DEBUG: '$outfile' contains $lines_written line(s)."
              if [ $lines_written -gt 0 ]; then
                echo "DEBUG: Sample lines from '$outfile':"
                head -n 3 "$outfile" || true
              else
                echo "DEBUG: '$outfile' is empty."
              fi
            else
              echo "DEBUG: '$outfile' was not created."
            fi
          done < parent_domains.txt
          echo "-> Split complete for domain '$DOMAIN'."
          # Always create a results/ directory (so upload-artifact never complains about “no files”)
          mkdir -p results
      
      - name: Compute SAFE_CHUNK (no slashes)
        run: |
          SAFE_CHUNK="${{ matrix.pair.chunk }}"
          SAFE_CHUNK="$(echo "$SAFE_CHUNK" | tr '/' '_')"
          echo "SAFE_CHUNK=$SAFE_CHUNK" >> $GITHUB_ENV         
      
      - name: Upload Primary Account puredns+dsieve Results
        uses: actions/upload-artifact@v4
        with:
          name: primary_purednss_dsieve_${{ matrix.pair.domain }}_${{ env.SAFE_CHUNK }}
          path: results/
          retention-days: 1
  
  commit_all_primary_results:
    name: Commit All Primary Results
    needs: resolve_primary_account_chunks
    if: always() 
    runs-on: ubuntu-latest
    container:
      image: ghcr.io/pcoder7/spider-puredns-actions:latest
      credentials:
        username: ${{ secrets.GHCR_USER }}
        password: ${{ secrets.GHCR_TOKEN }}        
    steps:
      - name: Checkout repository (Account 2's repo)
        uses: actions/checkout@v3
        with:
          token: ${{ secrets.GITHUB_TOKEN }}

      - name: Download all 'primary_purednss_dsieve_*' artifacts from this workflow run
        uses: actions/download-artifact@v4
        with:
          pattern: primary_purednss_dsieve_*
          path: temp_results

      - name: Organize and Push to store-recon
        shell: bash
        env:
          STORE_RECON_PAT: ${{ secrets.PAT_FOR_SECONDRY }}
          STORE: ${{ secrets.STORE }}
          ACCOUNT2_USERNAME: ${{ secrets.ACCOUNT2_REPO_OWNER }}
        run: |
          echo "Organizing primary results..."
          git config --global --add safe.directory "$(pwd)"
          # First, collect all chunk outputs into the runner’s results/ folder
          mkdir -p results
          find temp_results -type f -name "puredns_result.txt" | while read -r filepath; do
            parent="$(basename "$(dirname "$filepath")")"
            mkdir -p "results/$parent"
            cat "$filepath" | anew -q "results/$parent/puredns_result.txt"
            echo "WORKFLOW RUN: 'results/$parent/puredns_result.txt' contains $(wc -l < "results/$parent/puredns_result.txt") valid domains for parent $parent."
          done || true
          # If the runner’s results/ is empty, nothing to push
          if [ ! -d "results" ] || [ -z "$(ls -A results)" ]; then
            echo "No results to push."
            exit 0
          fi
          # Configure Git identity for commits
          git config --global user.name "Account2 PureDNS Bot"
          git config --global user.email "actions-bot@users.noreply.github.com"
          # Clone the private store-recon repo from Account 2
          TMP_DIR="$(mktemp -d)"
          echo "Cloning store-recon into $TMP_DIR"
          git clone "https://x-access-token:${STORE_RECON_PAT}@github.com/${ACCOUNT2_USERNAME}/${STORE}.git" "$TMP_DIR"
          cd "$TMP_DIR" || exit 1
          # Ensure a top-level results/ exists in the cloned repo
          mkdir -p results
          # Retry up to 10 times to handle races on main
          for i in {1..10}; do
            git fetch origin main
            git checkout main
            git pull --rebase origin main
                               
            
            WORKSPACE_RESULTS="${GITHUB_WORKSPACE}/results"
            find "$WORKSPACE_RESULTS" -type f -name "puredns_result.txt" | while read -r filepath; do
              rel="${filepath#"$WORKSPACE_RESULTS/"}"
              mkdir -p "results/$(dirname "$rel")"
              cat "$filepath" | anew -q "results/$rel"
              echo "DEBUG: 'results/$rel' now has $(wc -l < "results/$rel") lines."
            done
            git add results/
            if git diff --cached --quiet; then
              echo "No changes to commit in store-recon."
              exit 0
            fi
            git commit -m "Primary puredns+dsieve results (Run: ${{ github.event.inputs.primary_run_id || github.run_id }})"
            if git push origin main; then
              echo "Pushed to store-recon successfully on attempt #$i."
              exit 0
            else
              echo "Push failed on attempt #$i, retrying..."
              git reset --hard origin/main
            fi
          done
          echo "ERROR: Could not push results to store-recon after 10 attempts."
          exit 1      
      
