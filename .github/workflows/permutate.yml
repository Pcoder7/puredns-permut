on:
  workflow_dispatch:

permissions:
  contents: write   # For creating matrix.json, artifacts, and potentially committing aggregated results
  actions: write    # For triggering workflows in Account 2's repository

env:
  # --- Chunking and Distribution Configuration ---
  LINES_PER_CHUNK: 100000 # Unchanged from original
  DISTRIBUTION_THRESHOLD: 21    # NEW: The minimum total chunks to trigger 3-way distribution
  PRIMARY_PERCENTAGE: 45        # NEW: 30% for the primary account
  SECONDARY_PERCENTAGE: 40      # NEW: 30% for the secondary account

  # Define Account 2 repo details here or get from secrets for more flexibility
  # --- Account 2 Details ---
  ACCOUNT2_REPO_OWNER: ${{ secrets.ACCOUNT2_REPO_OWNER || 'pushrockzz' }} # Unchanged from original
  ACCOUNT2_REPO_NAME: ${{ secrets.ACCOUNT2_REPO_NAME || 'puredns-resolve' }} # Unchanged from original

  # --- Account 3 Details ---
  ACCOUNT3_REPO_OWNER: ${{ secrets.ACCOUNT3_REPO_OWNER || 'Sari2705' }}      # NEW
  ACCOUNT3_REPO_NAME: ${{ secrets.ACCOUNT3_REPO_NAME || 'puredns-resolve' }} # NEW

jobs:
  prepare_all_chunks_and_package:
    name: Prepare All Chunks & Package
    runs-on: ubuntu-latest
    container:
      image: ghcr.io/pcoder7/spider-puredns-actions:latest
      credentials:
        username: ${{ github.actor }}
        password: ${{ secrets.GHCR_TOKEN }}
    outputs:
      # Output 1: The full JSON matrix string of ALL generated chunks
      all_chunks_matrix_json: ${{ steps.build_full_matrix.outputs.full_matrix }}
      # Output 2: The total number of chunks generated
      total_chunks_count: ${{ steps.build_full_matrix.outputs.total_chunks }}
      # Output 3: The name of the artifact containing all chunks and resolvers
      chunk_package_artifact_name: "all-chunks-package-${{ github.run_id }}"
    steps:
      - name: Checkout repository
        uses: actions/checkout@v3
        with:
          token: ${{ secrets.GITHUB_TOKEN }}

      
      - name: Build Smart Tiered & Randomized Seed List from Store-Recon
        id: create_final_list
        shell: bash
        env:
          STORE_RECON_PAT: ${{ secrets.PAT_FOR_SECONDRY }}
          STORE: ${{ secrets.STORE }}
          ACCOUNT2_USERNAME: ${{ secrets.ACCOUNT2_REPO_OWNER }}
        run: |
          set -e
          echo "INFO: Performing a fast, shallow clone of the results repository..."
          # A shallow clone is fine here because we only need the current state, not git history.
          git clone --depth 1 "https://x-access-token:${STORE_RECON_PAT}@github.com/${ACCOUNT2_USERNAME}/${STORE}.git" store-recon-temp
          
          echo "INFO: Aggregating all results into a master list..."
          if [ -d "store-recon-temp/results" ]; then
              find store-recon-temp/results -type f -name "puredns_result.txt" -exec awk '{print $1}' {} + > all-resolved-subdomains.txt
          else
              echo "WARNING: 'results' directory not found. Exiting."
              exit 0
          fi

          if [ ! -s all-resolved-subdomains.txt ]; then
              echo "WARNING: Master list 'all-subdomains.txt' is empty. Exiting."
              exit 0
          fi

          echo "INFO: Extracting unique root domains from the master list..."
          dsieve -if all-resolved-subdomains.txt -f 2 | sort -u > unique_root_domains.txt
          
          # This will be our final list used for permutations
          > final-subdomain-list.txt

          echo "INFO: Building smart seed list for each root domain..."
          while read -r ROOT_DOMAIN; do
            if [ -z "$ROOT_DOMAIN" ]; then continue; fi
            echo "====================================================="
            echo "Processing Root Domain: $ROOT_DOMAIN"
            
            # Create a temporary file containing only subdomains for the current root domain
            TEMP_ROOT_LIST="${TMPDIR}/${ROOT_DOMAIN}-list.txt"
            grep -E "(^|\\.)${ROOT_DOMAIN//./\\.}(\$)" all-resolved-subdomains.txt --color=never > "$TEMP_ROOT_LIST" || true

            if [ ! -s "$TEMP_ROOT_LIST" ]; then continue; fi

            # This file will collect our seeds for THIS domain before we de-duplicate
            TEMP_SEED_LIST_DOMAIN="${TMPDIR}/${ROOT_DOMAIN}-seeds.txt"
            > "$TEMP_SEED_LIST_DOMAIN"
            
            # --- Your Idea, Part 1: Random Sample of the Top-Tier ---
            echo "  -> Getting a random 1000 sample from the top 2000 most common subdomains..."
            # We use -top-subdomains to get the full subdomains, then shuffle and take a sample.
            dsieve -if "$TEMP_ROOT_LIST" -top 2000 | shuf | head -n 1000 | anew -q "$TEMP_SEED_LIST_DOMAIN"
            
            # --- Your Idea, Part 2: Random Sample of the Mid-Tier ---
            echo "  -> Getting a random 500 sample from the 'mid-tier' (ranks 2001-5000)..."
            # This is your clever tail trick, translated to full subdomains.
            dsieve -if "$TEMP_ROOT_LIST" -top 5000 | tail -n 3000 | shuf | head -n 500 | anew -q "$TEMP_SEED_LIST_DOMAIN"

            # --- Finalize for this domain ---
            # Append the unique seeds for this domain to the master list
            SEED_COUNT=$(wc -l < "$TEMP_SEED_LIST_DOMAIN")
            echo "  -> Generated $SEED_COUNT potential seeds for $ROOT_DOMAIN."
            # Using cat and sort is often faster than piping to anew here.
            cat "$TEMP_SEED_LIST_DOMAIN" >> final-subdomain-list.txt

          done < unique_root_domains.txt
          
          # Final de-duplication of the entire list to ensure it's perfectly clean
          echo "INFO: Performing final de-duplication of the master seed list..."
          mv final-subdomain-list.txt final-subdomain-list.tmp
          sort -u final-subdomain-list.tmp > final-subdomain-list.txt
          rm final-subdomain-list.tmp

          FINAL_COUNT=$(wc -l < final-subdomain-list.txt)
          echo "====================================================="
          echo "SUCCESS: 'final-subdomain-list.txt' created with $FINAL_COUNT total unique seeds for permutation."
          
          # Clean up the cloned repository
          rm -rf store-recon-temp
      
      - name: Process, Generate, and Filter Permutations with Dynamic Wordlists
        env:
          MASTER_INPUT_LIST: "final-subdomain-list.txt" # More flexible way to set the input file
        shell: bash
        run: |
          # --- Setup, Input Files, and Tool Installation ---
          export TMPDIR="$RUNNER_TEMP"
          set -e # Exit immediately if a command exits with a non-zero status.

          echo "INFO: Verifying input file..."
          [ -f "$MASTER_INPUT_LIST" ] || { echo "CRITICAL: Missing input file '$MASTER_INPUT_LIST'."; exit 1; }

          

          echo "-----------------------------------------------------"

          # --- PRE-PROCESSING: Extract Unique Root Domains ---
          echo "PRE-PROCESSING: Extracting unique root domains from '$MASTER_INPUT_LIST'..."
          dsieve -if "$MASTER_INPUT_LIST" -f 2 | sort -u > unique_root_domains.txt
          ROOT_DOMAIN_COUNT=$(wc -l < unique_root_domains.txt)
          echo "INFO: Found $ROOT_DOMAIN_COUNT unique root domains to process."
          if [ "$ROOT_DOMAIN_COUNT" -eq 0 ]; then
            echo "WARNING: No root domains found. Exiting."
            touch subdomains.txt # Ensure file exists for subsequent steps
            exit 0
          fi
          echo "-----------------------------------------------------"



          # This file will hold all generated permutations before unique filtering
          > all_permutations_temp.txt

          # --- MAIN PROCESSING LOOP: Iterate over each root domain ---
          while read -r ROOT_DOMAIN; do
            if [ -z "$ROOT_DOMAIN" ]; then continue; fi
            echo "====================================================="
            echo "PROCESSING ROOT DOMAIN: $ROOT_DOMAIN"
            echo "====================================================="

            # Step 1: Create a temporary base list for the current root domain
            TEMP_BASE_LIST="${TMPDIR}/${ROOT_DOMAIN}-base.txt"
            echo "INFO: Creating temporary base list for '$ROOT_DOMAIN'..."
            grep -E "(^|\\.)${ROOT_DOMAIN//./\\.}(\$)" "$MASTER_INPUT_LIST" --color=never > "$TEMP_BASE_LIST" || true

            if [ ! -s "$TEMP_BASE_LIST" ]; then
                echo "WARNING: No subdomains found for '$ROOT_DOMAIN' in the master list. Skipping."
                continue
            fi

            # Step 2: Generate a dynamic, domain-specific wordlist
            DYNAMIC_WORDLIST="${TMPDIR}/${ROOT_DOMAIN}-wordlist.txt"
            echo "INFO: Generating dynamic wordlist for '$ROOT_DOMAIN'..."
            # This pipeline is efficient and correct
            dsieve -if "$TEMP_BASE_LIST" -f 3 -top 50| awk -F '[-.]' '{for(i=1;i<=NF;i++) print $i}' | sort -u | grep -vE '^[0-9]+$|^.$' --color=never | anew -q "$DYNAMIC_WORDLIST" || true
            
            #dsieve -if "$TEMP_BASE_LIST" -f 4 -top 500 | awk -F '[-.]' '{for(i=1;i<=NF;i++) print $i}' | sort -u | grep -vE '^[0-9]+$|^.$' | anew -q "$DYNAMIC_WORDLIST"
            #dsieve -if "$TEMP_BASE_LIST" -f 5 -top 200 | awk -F '[-.]' '{for(i=1;i<=NF;i++) print $i}' | sort -u | grep -vE '^[0-9]+$|^.$' | anew -q "$DYNAMIC_WORDLIST"
            
            DYNAMIC_WORD_COUNT=$(wc -l < "$DYNAMIC_WORDLIST")
            if [ "$DYNAMIC_WORD_COUNT" -eq 0 ]; then
                echo "WARNING: Could not generate a dynamic wordlist for '$ROOT_DOMAIN'. Skipping permutation tools."
                continue
            fi
            echo "INFO: Generated dynamic wordlist with $DYNAMIC_WORD_COUNT words."

            # Step 3: Run permutation tools efficiently, once per tool per domain

                
                echo "  -> Running gotator with dynamic wordlist..."
                # THE FIX: Build the command string first to handle quotes correctly.
                # We use 'printf' with '%q' to safely quote the wordlist path.
                SAFELY_QUOTED_WORDLIST=$(printf '%q' "$DYNAMIC_WORDLIST")


                # Stage 3a: Dnsgen
                echo "  -> Running dnsgen with dynamic wordlist..."
                DNSGEN_CMD="dnsgen -w $SAFELY_QUOTED_WORDLIST {} > {}-dnsgen.txt"

                chrunk -p 20 -i "$TEMP_BASE_LIST" -c 20 -prefix "${ROOT_DOMAIN}-dnsgen" -cmd "$DNSGEN_CMD"
                
                DNSGEN_RESULTS_TMP="${TMPDIR}/${ROOT_DOMAIN}-dnsgen-results.txt"
                cat "$TMPDIR"/chrunk-data/${ROOT_DOMAIN}-dnsgen-*.txt 2>/dev/null > "$DNSGEN_RESULTS_TMP"
                DNSGEN_UNIQUE_ADDITIONS=$(cat "$DNSGEN_RESULTS_TMP" | sort -u | wc -l)
                echo "    - Dnsgen generated $DNSGEN_UNIQUE_ADDITIONS found new unique subdomains."
                cat "$DNSGEN_RESULTS_TMP" | anew -q all_permutations_temp.txt
                
                # Stage 3b: Alterx
                echo "  -> Running alterx with dynamic wordlist..."
               
                # 'printf' also works for the alterx template path
                SAFELY_QUOTED_TEMPLATE=$(printf '%q' "template.txt")
                ALTERX_CMD="alterx -l {} -enrich -p $SAFELY_QUOTED_TEMPLATE -pp 'word=$SAFELY_QUOTED_WORDLIST' > {}-alterx.txt"
                
                chrunk -p 20 -i "$TEMP_BASE_LIST" -c 20 -prefix "${ROOT_DOMAIN}-alterx" -cmd "$ALTERX_CMD"

                ALTERX_RESULTS_TMP="${TMPDIR}/${ROOT_DOMAIN}-alterx-results.txt"
                cat "$TMPDIR"/chrunk-data/${ROOT_DOMAIN}-alterx-*.txt 2>/dev/null > "$ALTERX_RESULTS_TMP"
                ALTERX_UNIQUE_ADDITIONS=$(cat "$ALTERX_RESULTS_TMP" | anew -d all_permutations_temp.txt | wc -l)
                echo "    - Alterx found $ALTERX_UNIQUE_ADDITIONS new unique subdomains."
                cat "$ALTERX_RESULTS_TMP" | anew -q all_permutations_temp.txt

                # Stage 3b: Alterx
                
                #GOTATOR_CMD="gotator-solved -sub {} -depth 1 -adv -perm '"$DYNAMIC_WORDLIST"' -mindup -silent | anew -q {}-gotator.txt"
                
                chrunk -p 20 -i "$TEMP_BASE_LIST" -c 20 -cmd 'gotator-solved -sub "{}" -depth 1 -perm '"$DYNAMIC_WORDLIST"' -adv -mindup -silent | anew -q "{}-gotator.txt"' -prefix "${ROOT_DOMAIN}-gotator"
                
                GOTATOR_RESULTS_TMP="${TMPDIR}/${ROOT_DOMAIN}-gotator-results.txt"
                cat "$TMPDIR"/chrunk-data/${ROOT_DOMAIN}-gotator-*.txt 2>/dev/null > "$GOTATOR_RESULTS_TMP"
                
                wc -l "$GOTATOR_RESULTS_TMP"
                GOTATOR_COUNT=$(cat "$GOTATOR_RESULTS_TMP" | anew -d all_permutations_temp.txt | wc -l)
                echo "    - Gotator found $GOTATOR_COUNT new unique subdomains"
                cat "$GOTATOR_RESULTS_TMP" | anew -q all_permutations_temp.txt
              
            
            echo "INFO: Finished processing for $ROOT_DOMAIN."
            
            echo "INFO: Finished processing for $ROOT_DOMAIN."
            # Clean up chrunk's temporary output directories for this root domain
            
          done < unique_root_domains.txt

          # Now, de-duplicate all results at once
          echo "INFO: De-duplicating all generated permutations..."
          cat all_permutations_temp.txt | anew -q raw_permutations.txt
          rm all_permutations_temp.txt

          echo "====================================================="

          # --- FINAL FILTERING STAGE ---
          echo "FINAL FILTERING: Removing jargon and complex numeric/IP-like patterns..."

          # First, ensure the files we need actually exist
          if [ ! -s "raw_permutations.txt" ]; then
            echo "WARNING: raw_permutations.txt is empty or does not exist. No filtering will be done."
            touch subdomains.txt # Create an empty file to ensure downstream steps don't fail
            exit 0
          fi
          if [ ! -f "filter.txt" ]; then
            echo "CRITICAL ERROR: filter.txt not found. Cannot apply filters. Aborting."
            exit 1
          fi

          RAW_COUNT=$(wc -l < raw_permutations.txt)
          echo "INFO: Starting with $RAW_COUNT raw permutations."

          # --- THE FIX: Sanitize and apply the filter file ---
          echo "INFO: Sanitizing filter.txt to remove empty lines or Windows line endings..."
          
          # This creates a clean version of the filter file that grep can safely use.
          # It removes any empty lines and converts potential \r\n to \n.
          
          grep -v '^$' filter.txt --color=never  | sed 's/\r$//' > sanitized_filters.txt || true

          echo "INFO: Applying filters from sanitized_filters.txt..."

          grep -ivE -f sanitized_filters.txt raw_permutations.txt --color=never > subdomains.txt || true

          # Clean up the temporary sanitized file
          rm sanitized_filters.txt

          FINAL_COUNT=$(wc -l < subdomains.txt)
          FILTERED_COUNT=$((RAW_COUNT - FINAL_COUNT))

          echo "INFO: Filtered out $FILTERED_COUNT subdomains."
          echo "FINAL: The cleaned 'subdomains.txt' now contains $FINAL_COUNT permutations."
          rm -rf "$TMPDIR"/chrunk-data/${ROOT_DOMAIN}-*
          cat raw_permutations.txt | anew -q subdomains.txt
          wc -l subdomains.txt
          
      - name: DEBUG - Verify Tools and Initial File System State
        shell: bash
        run: |
          echo "DEBUG: Current directory: $(pwd)"
          echo "DEBUG: Listing all files in repository root and subdirectories:"
          find . -print
          echo "-----------------------------------------------------"
          echo "DEBUG: Specifically looking for subdomains.txt files:"
          find . -type f -name 'subdomains.txt' -print || echo "DEBUG: 'find' for subdomains.txt failed or found nothing."
          echo "-----------------------------------------------------"
          echo "DEBUG: Checking for 'jq' command..."
          if ! command -v jq &> /dev/null; then
            echo "CRITICAL_ERROR: 'jq' command not found. This is essential. Aborting."
            exit 1
          else
            echo "DEBUG: jq version: $(jq --version)"
          fi
          echo "-----------------------------------------------------"
          echo "DEBUG: Checking for 'split' command..."
          if ! command -v split &> /dev/null; then
            echo "CRITICAL_ERROR: 'split' command not found. This is essential. Aborting."
            exit 1
          else
            echo "DEBUG: split version information (if available):"
            split --version || echo "DEBUG: Could not get split version."
          fi
          echo "-----------------------------------------------------"
      - name: Build Full Matrix & Create Chunks
        id: build_full_matrix
        shell: bash
        run: |
          # --- Start of Script ---
          JSON_MATRIX='[]' # Initialize as an empty JSON array string
          echo "INFO: Initializing JSON_MATRIX as: '$JSON_MATRIX'"
          echo "INFO: Locating 'subdomains.txt' files..."
          find . -type f -name "subdomains.txt" -print0 > found_files.tmp
          declare -a files=() # Explicitly declare as an array
          while IFS= read -r -d $'\0' file_path_from_find; do
            standardized_file_path=$(echo "$file_path_from_find" | sed 's|^\./||')
            files+=("$standardized_file_path")
          done < found_files.tmp
          rm found_files.tmp # Clean up temporary file
          if [ "${#files[@]}" -eq 0 ]; then
            echo "WARNING: No 'subdomains.txt' files found in the repository."
            echo "INFO: JSON_MATRIX will remain '[]'. No chunks will be generated."
          else
            echo "INFO: Found ${#files[@]} 'subdomains.txt' file(s):"
            printf "  => '%s'\n" "${files[@]}" # Print each found file for clarity
            for file_path in "${files[@]}"; do
              echo "-----------------------------------------------------"
              echo "INFO: Processing file: '$file_path'"
              domain_dir=$(dirname "$file_path")
              if [ "$domain_dir" == "." ]; then
                domain=$(basename "$file_path" .txt)
              else
                domain=$(basename "$domain_dir")
              fi
              echo "INFO: Deduced domain: '$domain'"
              if [ ! -f "$file_path" ]; then
                echo "ERROR: File '$file_path' reported by find, but not found now. Skipping."
                continue
              fi
              if [ ! -s "$file_path" ]; then
                echo "WARNING: File '$file_path' is empty. Skipping splitting for this file."
                continue
              fi
              echo "INFO: Creating chunk directory 'chunks/$domain' if it doesn't exist."
              mkdir -p "chunks/$domain"
              output_chunk_prefix="chunks/$domain/chunk_"
              echo "INFO: Splitting '$file_path' into chunks in 'chunks/$domain/' with prefix '$output_chunk_prefix' (max $LINES_PER_CHUNK lines/chunk)"
              
              split -l "$LINES_PER_CHUNK" -a 3 --numeric-suffixes=1 "$file_path" "$output_chunk_prefix"
              split_exit_code=$?
              if [ $split_exit_code -ne 0 ]; then
                echo "ERROR: 'split' command failed with exit code $split_exit_code for file '$file_path'. Skipping chunk generation for this file."
                continue
              fi
              
              CHUNK_COUNT_FOR_THIS_FILE=0
              while IFS= read -r chunk_file_path; do
                if [ -z "$chunk_file_path" ]; then continue; fi
                echo "DEBUG: Processing chunk_file_path: '$chunk_file_path' for domain '$domain'"
                echo "DEBUG: Current JSON_MATRIX before jq (first 100 chars): $(echo "$JSON_MATRIX" | head -c 100)"
                echo "DEBUG: Validating current JSON_MATRIX with jq -e:"
                if echo "$JSON_MATRIX" | jq -e . > /dev/null 2>&1; then
                  echo "DEBUG: Current JSON_MATRIX is VALID."
                else
                  echo "CRITICAL_ERROR_DETECTED: Current JSON_MATRIX is INVALID before processing '$chunk_file_path'."
                  echo "CRITICAL_ERROR_DETECTED: JSON_MATRIX content: $JSON_MATRIX"
                  if [ "$JSON_MATRIX" != "[]" ] && [ "$CHUNK_COUNT_FOR_THIS_FILE" -gt 0 ]; then
                      echo "ABORTING chunk addition for this file due to previously corrupted JSON_MATRIX."
                      break 
                  else
                      echo "WARNING: JSON_MATRIX was invalid but appeared to be at an initial state. Attempting to reset to []."
                      JSON_MATRIX="[]"
                  fi
                fi
                
                if ! command -v jq &> /dev/null; then
                  echo "CRITICAL_ERROR: 'jq' command not found during chunk processing. Aborting this file's chunk addition."
                  break 
                fi
                
                echo "DEBUG: jq command to be run: printf '%s' \"\$JSON_MATRIX\" | jq -c --arg d \"$domain\" --arg c \"$chunk_file_path\" '. + [{domain:\$d,chunk:\$c}]'"
                TEMP_JSON_MATRIX=$(printf '%s' "$JSON_MATRIX" | jq -c --arg d "$domain" --arg c "$chunk_file_path" '. + [{domain:$d,chunk:$c}]')
                jq_exit_code=$?
                if [ $jq_exit_code -ne 0 ]; then
                  echo "ERROR: jq command failed (exit code $jq_exit_code) when trying to add chunk '$chunk_file_path' for domain '$domain'."
                  echo "ERROR: Input JSON_MATRIX was (first 100 chars): $(echo "$JSON_MATRIX" | head -c 100)"
                  echo "ERROR: Arguments to jq were: domain='$domain', chunk_file_path='$chunk_file_path'"
                  continue
                elif [ -z "$TEMP_JSON_MATRIX" ]; then
                  echo "ERROR: jq command produced empty output for chunk '$chunk_file_path'. This should not happen with '. + [...]'."
                  echo "ERROR: Input JSON_MATRIX that led to empty output (first 200 chars): $(echo "$JSON_MATRIX" | head -c 200)"
                  echo "ERROR: Full input JSON_MATRIX that led to empty output: $JSON_MATRIX"
                  echo "ERROR: Arguments to jq were: domain='$domain', chunk_file_path='$chunk_file_path'"
                  echo "DEBUG: Re-validating the input JSON_MATRIX that caused empty output:"
                  if echo "$JSON_MATRIX" | jq -e . > /dev/null 2>&1; then
                      echo "DEBUG: Input JSON_MATRIX was still considered VALID by jq -e just before the empty output. This is very strange."
                  else
                      echo "DEBUG: Input JSON_MATRIX was considered INVALID by jq -e. This is the likely cause of empty output."
                  fi
                  if [ "$JSON_MATRIX" == "" ]; then
                      echo "DEBUG: JSON_MATRIX was an empty string. Trying to initialize with the current chunk."
                      TEMP_JSON_MATRIX=$(jq -cn --arg d "$domain" --arg c "$chunk_file_path" '[{domain:$d,chunk:$c}]')
                      if [ -n "$TEMP_JSON_MATRIX" ]; then
                          echo "DEBUG: Successfully initialized TEMP_JSON_MATRIX from empty string case."
                      else
                          echo "ERROR: Still failed to initialize TEMP_JSON_MATRIX even from empty string case."
                          continue
                      fi
                  else
                      continue 
                  fi
                fi
                JSON_MATRIX="$TEMP_JSON_MATRIX"
                CHUNK_COUNT_FOR_THIS_FILE=$((CHUNK_COUNT_FOR_THIS_FILE + 1))
                echo "DEBUG: Successfully added/updated for chunk '$chunk_file_path'. JSON_MATRIX (first 100 chars): $(echo "$JSON_MATRIX" | head -c 100)"
              done < <(find "chunks/$domain/" -name 'chunk_*' -type f -print)
              if [ "$CHUNK_COUNT_FOR_THIS_FILE" -eq 0 ]; then
                echo "WARNING: No chunk files were processed/added to matrix for '$file_path' in domain '$domain'. 'split' might have created no files, or 'jq' failed for all."
              else
                echo "INFO: Processed and added $CHUNK_COUNT_FOR_THIS_FILE chunk(s) to matrix for domain '$domain' from '$file_path'."
              fi
            done
          fi
          echo "-----------------------------------------------------"
          if ! echo "$JSON_MATRIX" | jq -e . > /dev/null 2>&1; then
            echo "ERROR: Final JSON_MATRIX is not valid JSON. Content (first 200 chars): $(echo "$JSON_MATRIX" | head -c 200)"
            echo "WARNING: Setting JSON_MATRIX to '[]' and total_chunks to 0 due to invalid JSON."
            JSON_MATRIX="[]"
            TOTAL_CHUNKS=0
          else
            TOTAL_CHUNKS=$(echo "$JSON_MATRIX" | jq 'length')
          fi
          echo "FINAL_BUILD_INFO: Final JSON_MATRIX content (first 200 chars): $(echo "$JSON_MATRIX" | head -c 200)"
          echo "FINAL_BUILD_INFO: Total chunks in matrix: $TOTAL_CHUNKS"
          echo "INFO: Writing the final matrix to 'full_matrix.json' file..."
          echo "$JSON_MATRIX" > full_matrix.json
          if [ -f full_matrix.json ]; then
            echo "VERIFY: 'full_matrix.json' created. Size: $(wc -c < full_matrix.json) bytes."
            echo "VERIFY: Content preview of 'full_matrix.json' (first 200 chars): $(head -c 200 full_matrix.json)"
            if jq -e . full_matrix.json > /dev/null 2>&1; then
              echo "VERIFY: 'full_matrix.json' contains valid JSON."
            else
              echo "ERROR: 'full_matrix.json' does NOT contain valid JSON! This will cause issues for 'fromJson' later."
            fi
          else
            echo "CRITICAL_ERROR: 'full_matrix.json' was NOT created after attempting to write JSON_MATRIX."
            echo "INFO: Forcing 'full_matrix.json' to be '[]' as a fallback to prevent tar error."
            echo "[]" > full_matrix.json
          fi
          echo "full_matrix=$JSON_MATRIX" >> $GITHUB_OUTPUT
          echo "total_chunks=$TOTAL_CHUNKS" >> $GITHUB_OUTPUT                 
     
      - name: Package All Chunks and Resolvers      
        id: package_chunks # This ID is referenced by outputs and later steps
        if: steps.build_full_matrix.outputs.total_chunks > 0
        shell: bash
        run: |
          # Define the base name for the artifact (without .tar.gz)
          BASE_ARTIFACT_NAME="all-chunks-package-${{ github.run_id }}"
          # Define the full tar filename
          PACKAGE_TAR_FILENAME="${BASE_ARTIFACT_NAME}.tar.gz"
          echo "INFO: Preparing package '$PACKAGE_TAR_FILENAME'..."
          # Ensure resolver files and full_matrix.json exist, create fallbacks if not
          if [ ! -f resolvers.txt ]; then echo "WARNING: resolvers.txt not found, creating default." >&2; echo '1.1.1.1' > resolvers.txt; fi
          if [ ! -f resolvers-trusted.txt ]; then echo "WARNING: resolvers-trusted.txt not found, creating empty." >&2; touch resolvers-trusted.txt; fi
          if [ ! -f full_matrix.json ]; then echo "WARNING: full_matrix.json not found, creating empty array JSON file." >&2; echo "[]" > full_matrix.json; fi
          
          # Create the tarball
          echo "INFO: Creating tarball: $PACKAGE_TAR_FILENAME"
          tar -czvf "$PACKAGE_TAR_FILENAME" chunks resolvers.txt resolvers-trusted.txt full_matrix.json
          
          if [ $? -eq 0 ]; then
            echo "INFO: '$PACKAGE_TAR_FILENAME' created successfully."
          else
            echo "ERROR: Failed to create tarball '$PACKAGE_TAR_FILENAME'."
            # Exit or handle error appropriately if tar fails
            exit 1 
          fi
          
          # Output the full .tar.gz filename for the 'path' in upload-artifact
          echo "package_tar_filename=$PACKAGE_TAR_FILENAME" >> $GITHUB_OUTPUT
          # Output the base artifact name (without .tar.gz) for the 'name' in upload-artifact and for job output
          echo "base_artifact_name=$BASE_ARTIFACT_NAME" >> $GITHUB_OUTPUT
      
      - name: Upload Full Chunks Package
        if: steps.build_full_matrix.outputs.total_chunks > 0 && steps.package_chunks.outcome == 'success'
        uses: actions/upload-artifact@v4
        with:
          # Use the base artifact name output from the 'package_chunks' step
          name: ${{ steps.package_chunks.outputs.base_artifact_name }}
          # Use the full tar filename output from the 'package_chunks' step
          path: ${{ steps.package_chunks.outputs.package_tar_filename }}
          retention-days: 1

  distribute_and_trigger_secondary:
    name: Distribute Work & Trigger Secondary
    needs: prepare_all_chunks_and_package
    if: needs.prepare_all_chunks_and_package.outputs.total_chunks_count > 0 # Only run if there are chunks
    runs-on: ubuntu-latest
    outputs:
      primary_matrix_json: ${{ steps.calculate_distribution.outputs.primary_matrix }}
      secondary_matrix_json: ${{ steps.calculate_distribution.outputs.secondary_matrix }}
      tertiary_matrix_json: ${{ steps.calculate_distribution.outputs.tertiary_matrix }}
      secondary_processing_triggered: ${{ steps.trigger_secondary.outcome == 'success' && steps.calculate_distribution.outputs.secondary_chunks_exist == 'true' }}
      tertiary_processing_triggered: ${{ steps.trigger_tertiary.outcome == 'success' && steps.calculate_distribution.outputs.tertiary_chunks_exist == 'true' }}
   
    steps:
      - name: Calculate Chunk Distribution for Accounts
        id: calculate_distribution
        shell: bash
        run: |
          ALL_CHUNKS_JSON='${{ needs.prepare_all_chunks_and_package.outputs.all_chunks_matrix_json }}'
          TOTAL_CHUNKS=${{ needs.prepare_all_chunks_and_package.outputs.total_chunks_count }}
          THRESHOLD=${{ env.DISTRIBUTION_THRESHOLD }}

          echo "Total chunks generated: $TOTAL_CHUNKS"
          echo "Distribution threshold: $THRESHOLD"

          if [ "$TOTAL_CHUNKS" -lt "$THRESHOLD" ]; then
            # --- STRATEGY 1: Below Threshold -> Assign all to Primary ---
            echo "Total chunks are below threshold. Assigning all work to Primary Account."
            PRIMARY_CHUNKS_JSON="$ALL_CHUNKS_JSON"
            SECONDARY_CHUNKS_JSON="[]"
            TERTIARY_CHUNKS_JSON="[]"
          else
            # --- STRATEGY 2: At or Above Threshold -> Apply Percentage Split ---
            echo "Total chunks are at or above threshold. Applying 30-30-40 percentage split."
            PRIMARY_PERCENT=${{ env.PRIMARY_PERCENTAGE }}
            SECONDARY_PERCENT=${{ env.SECONDARY_PERCENTAGE }}

            PRIMARY_CHUNK_COUNT=$(echo "($TOTAL_CHUNKS * $PRIMARY_PERCENT) / 100" | bc)
            SECONDARY_CHUNK_COUNT=$(echo "($TOTAL_CHUNKS * $SECONDARY_PERCENT) / 100" | bc)

            echo "Calculated counts: Primary=$PRIMARY_CHUNK_COUNT, Secondary=$SECONDARY_CHUNK_COUNT, Tertiary gets the rest."
            OFFSET_FOR_TERTIARY=$((PRIMARY_CHUNK_COUNT + SECONDARY_CHUNK_COUNT))

            PRIMARY_CHUNKS_JSON=$(echo "$ALL_CHUNKS_JSON" | jq -c --argjson limit "$PRIMARY_CHUNK_COUNT" '.[0:$limit]')
         
            SECONDARY_CHUNKS_JSON=$(echo "$ALL_CHUNKS_JSON" | jq -c --argjson offset "$PRIMARY_CHUNK_COUNT" --argjson limit "$SECONDARY_CHUNK_COUNT" '.[$offset : $offset+$limit]')
                   
          
            TERTIARY_CHUNKS_JSON=$(echo "$ALL_CHUNKS_JSON" | jq -c --argjson offset "$OFFSET_FOR_TERTIARY" '.[$offset:]')
 
          fi
          # Set outputs based on the chosen strategy
          echo "primary_matrix=$PRIMARY_CHUNKS_JSON" >> $GITHUB_OUTPUT
          
          if [ "$(echo "$SECONDARY_CHUNKS_JSON" | jq 'length')" -eq 0 ]; then
            echo "secondary_matrix=[]" >> $GITHUB_OUTPUT; echo "secondary_chunks_exist=false" >> $GITHUB_OUTPUT
          else
            echo "secondary_matrix=$SECONDARY_CHUNKS_JSON" >> $GITHUB_OUTPUT; echo "secondary_chunks_exist=true" >> $GITHUB_OUTPUT
          fi
          
          if [ "$(echo "$TERTIARY_CHUNKS_JSON" | jq 'length')" -eq 0 ]; then
            echo "tertiary_matrix=[]" >> $GITHUB_OUTPUT; echo "tertiary_chunks_exist=false" >> $GITHUB_OUTPUT
          else
            echo "tertiary_matrix=$TERTIARY_CHUNKS_JSON" >> $GITHUB_OUTPUT; echo "tertiary_chunks_exist=true" >> $GITHUB_OUTPUT
          fi
          echo "Distribution calculation complete."
     
      #====================================================================
      # ADD THE FIRST SLEEP STEP HERE
      # ====================================================================
      - name: Add delay before triggering Secondary worker
        if: steps.calculate_distribution.outputs.secondary_chunks_exist == 'true'
        run: |
          echo "Pausing for 15 seconds to space out worker triggers and avoid platform throttling..."
          sleep 15
      # ====================================================================

      
      - name: Prepare Trigger Payload For Secondary
        id: prepare_payload_secondary
        if: steps.calculate_distribution.outputs.secondary_chunks_exist == 'true'
        shell: bash
        run: |
          # Ensure the secondary_matrix output from the previous step is treated as a string
          # that already represents a JSON array. jq's --argjson will parse it.
          MATRIX_AS_STRING='${{ steps.calculate_distribution.outputs.secondary_matrix }}'
          # Validate that MATRIX_AS_STRING is actually valid JSON before --argjson
          if ! echo "${MATRIX_AS_STRING}" | jq -e . > /dev/null 2>&1; then
            echo "::error title=Invalid Secondary Matrix::The secondary_matrix output ('${MATRIX_AS_STRING}') is not valid JSON. Defaulting to empty array."
            MATRIX_AS_STRING="[]" # Fallback to an empty JSON array string
          fi
          # Construct the entire payload as a valid JSON string using jq
          # --argjson tells jq to parse the value of matrix_json as JSON, not treat it as a literal string
          JSON_PAYLOAD=$(jq -cn \
            --arg run_id "${{ github.run_id }}" \
            '{
              "primary_run_id": $run_id
            }')
             echo "Constructed JSON Payload: $JSON_PAYLOAD"
             echo "json_string=$JSON_PAYLOAD" >> "$GITHUB_OUTPUT"
           
          JSON_PAYLOAD=$(jq -cn \
            --arg server_url "${{ github.server_url }}" \
            --arg repo_owner "${{ github.repository_owner }}" \
            --arg repo_name "${{ github.event.repository.name }}" \
            --arg run_id "${{ github.run_id }}" \
            --arg artifact_name "${{ needs.prepare_all_chunks_and_package.outputs.chunk_package_artifact_name }}" \
            --arg matrix_as_a_string "${MATRIX_AS_STRING}" \
            '{
              "primary_github_server_url": $server_url,
              "primary_repo_owner": $repo_owner,
              "primary_repo_name": $repo_name,
              "primary_run_id": $run_id,
              "chunk_package_artifact_name": $artifact_name,
              "secondary_matrix_json": $matrix_as_a_string 
            }')
          
          echo "Constructed JSON Payload: $JSON_PAYLOAD"
          # Set the constructed JSON string as an output of this step
          echo "json_string=$JSON_PAYLOAD" >> $GITHUB_OUTPUT
      
      - name: DEBUG - Show Secondary Inputs String
        if: steps.calculate_distribution.outputs.secondary_chunks_exist == 'true'
        run: |
          echo "DEBUG: Inputs for SECONDARY account:"
          #echo "${{ steps.prepare_payload_secondary.outputs.json_string }}"
      
      - name: Trigger Secondary Account Workflow
        id: trigger_secondary 
        if: steps.calculate_distribution.outputs.secondary_chunks_exist == 'true'
        uses: benc-uk/workflow-dispatch@v1
        with:
          workflow: resolve.yml 
          repo: ${{ env.ACCOUNT2_REPO_OWNER }}/${{ env.ACCOUNT2_REPO_NAME }}
          token: ${{ secrets.PAT_FOR_SECONDARY_ACCOUNT_REPO }}
          inputs: ${{ steps.prepare_payload_secondary.outputs.json_string }}
          ref: main # Or your default branch name in Account 2

      #====================================================================
      # ADD THE FIRST SLEEP STEP HERE
      # ====================================================================
      - name: Add delay before triggering Tertiary Trigger worker
        if: steps.calculate_distribution.outputs.tertiary_chunks_exist == 'true'
        run: |
          echo "Pausing for 30 seconds to space out worker triggers and avoid platform throttling..."
          sleep 30
      # ====================================================================
      - name: Prepare Trigger Payload For Tertiary
        # This is a NEW step, a copy of the secondary payload step for Account 3.
        id: prepare_payload_tertiary
        if: steps.calculate_distribution.outputs.tertiary_chunks_exist == 'true'
        shell: bash
        run: |
          MATRIX_AS_STRING='${{ steps.calculate_distribution.outputs.tertiary_matrix }}'
          JSON_PAYLOAD=$(jq -cn \
            --arg run_id "${{ github.run_id }}" \
            '{
              "primary_run_id": $run_id
            }')
             echo "Constructed JSON Payload: $JSON_PAYLOAD"
             echo "json_string=$JSON_PAYLOAD" >> "$GITHUB_OUTPUT"
           
          JSON_PAYLOAD=$(jq -cn \
            --arg server_url "${{ github.server_url }}" \
            --arg repo_owner "${{ github.repository_owner }}" \
            --arg repo_name "${{ github.event.repository.name }}" \
            --arg run_id "${{ github.run_id }}" \
            --arg artifact_name "${{ needs.prepare_all_chunks_and_package.outputs.chunk_package_artifact_name }}" \
            --arg matrix_as_a_string "${MATRIX_AS_STRING}" \
            '{
              "primary_github_server_url": $server_url,
              "primary_repo_owner": $repo_owner,
              "primary_repo_name": $repo_name,
              "primary_run_id": $run_id,
              "chunk_package_artifact_name": $artifact_name,
              "tertiary_matrix_json": $matrix_as_a_string 
            }')
          
          echo "Constructed JSON Payload: $JSON_PAYLOAD"
          # Set the constructed JSON string as an output of this step
          echo "json_string=$JSON_PAYLOAD" >> $GITHUB_OUTPUT

      - name: DEBUG - Show Tertiary Inputs String
        if: steps.calculate_distribution.outputs.tertiary_chunks_exist == 'true'
        run: |
          echo "DEBUG: Inputs for TERTIARY account:"
          # echo "${{ steps.prepare_payload_tertiary.outputs.json_string }}"    

      - name: Trigger Tertiary Account Workflow
        # This is a NEW step, a copy of the secondary trigger step for Account 3.
        id: trigger_tertiary
        if: steps.calculate_distribution.outputs.tertiary_chunks_exist == 'true'
        uses: benc-uk/workflow-dispatch@v1
        with:
          workflow: resolve.yml
          repo: ${{ env.ACCOUNT3_REPO_OWNER }}/${{ env.ACCOUNT3_REPO_NAME }}
          token: ${{ secrets.PAT_FOR_TERTIARY_ACCOUNT_REPO }}
          inputs: ${{ steps.prepare_payload_tertiary.outputs.json_string }}
          ref: main


  resolve_primary_account_chunks:
    name: Resolve Primary Account Chunks (puredns + dsieve)
    needs: [prepare_all_chunks_and_package, distribute_and_trigger_secondary] # Depends on distribution logic
    if: needs.prepare_all_chunks_and_package.outputs.total_chunks_count > 0 && needs.distribute_and_trigger_secondary.outputs.primary_matrix_json != '[]'
    runs-on: ubuntu-latest
    container:
      image: ghcr.io/pcoder7/spider-puredns-actions:latest
      credentials:
        username: ${{ github.actor }}
        password: ${{ secrets.GHCR_TOKEN }}    
    strategy:
      fail-fast: false
      max-parallel: 20 
      matrix:
        pair: ${{ fromJson(needs.distribute_and_trigger_secondary.outputs.primary_matrix_json) }}
    steps:
      - name: Checkout repository (for results structure)
        uses: actions/checkout@v3

      - name: Download Full Chunks Package for Primary
        uses: actions/download-artifact@v4
        with:
          name: ${{ needs.prepare_all_chunks_and_package.outputs.chunk_package_artifact_name }}
          # Downloads to current directory

      - name: Extract Chunks for Primary
        shell: bash
        run: |
          PACKAGE_FILENAME="${{ needs.prepare_all_chunks_and_package.outputs.chunk_package_artifact_name }}.tar.gz"
          echo "Extracting $PACKAGE_FILENAME..."
          tar -xzvf "$PACKAGE_FILENAME"
          if [ ! -d "chunks" ]; then
             echo "ERROR: 'chunks/' not found after extraction!"
             exit 1
          fi
          echo "Extraction complete. 'chunks/' should be present."
          ls -R chunks/
      - name: Install dsieve
        run: |
          if command -v dsieve &> /dev/null; then
            echo "dsieve is already installed"
          else
            echo "Installing dsieve..."
            go install github.com/trickest/dsieve@latest
          fi
     
      - name: Install Tools
        run: |
          # Installing smap
          if ! command -v smap >/dev/null; then
            echo "Installing smap…"
            go install -v github.com/s0md3v/smap/cmd/smap@latest
          else
            echo "smap already in cache"
          fi    
          # Installing inscope
          if ! command -v inscope >/dev/null; then
            echo "Installing inscope…"
            go install -v github.com/tomnomnom/hacks/inscope@latest
          else
            echo "inscope already in cache"
          fi    
          
          if ! command -v anew >/dev/null; then
            echo "Installing anew…"
            go install -v github.com/tomnomnom/anew@latest
          else
            echo "anew already in cache"
          fi
          
          if ! command -v cut-cdn >/dev/null; then
            echo "Installing cut-cdn…"
            go install github.com/ImAyrix/cut-cdn@latest
          else
            echo "cut-cdn already in cache"
          fi     

          if ! command -v naabu >/dev/null; then
            echo "Installing naabu…"
            go install -v github.com/projectdiscovery/naabu/v2/cmd/naabu@latest
          else
            echo "naabu already in cache"
          fi

          pip3 install --no-cache-dir ipaddress
          
          echo "$HOME/go/bin" >> $GITHUB_PATH
          
  
      - name: Fetch wordlists
        shell: bash
        run: |
          if [ ! -f resolvers.txt ]; then
              wget -qO resolvers.txt \
              https://raw.githubusercontent.com/rix4uni/resolvers/refs/heads/main/resolvers.txt
              echo "resolvers.txt is downloaded"
          fi
          if [ ! -f resolvers-trusted.txt ]; then
              wget -qO resolvers-trusted.txt \
              https://raw.githubusercontent.com/and0x00/resolvers.txt/refs/heads/main/resolvers.txt
              echo "resolvers-trusted.txt is downloaded"
          fi          
     
      - name: Run puredns + pre-dsieve on subdomains + final filtering
        shell: bash
        run: |
          set -euo pipefail
          trap '' SIGPIPE

          CHUNK_FILE_PATH=${{ matrix.pair.chunk }}
          echo "Processing chunk '$CHUNK_FILE_PATH'..."
          if [ ! -f "$CHUNK_FILE_PATH" ]; then
            echo "ERROR: Chunk file '$CHUNK_FILE_PATH' not found!"
            exit 1
          fi

          # ====================================================================
          # STAGE 1: PUREDNS RESOLUTION (Naming now matches reference)
          # ====================================================================
          # Define filenames to match the reference workflow
          PUREDNS_FILE="puredns_file.txt"
          MASSDNS="massdns.txt"
          MASSDNS_FILE="massdns_file.txt"
          PARENT_DOMAINS_SCOPE="parent_domains.scope" # This is the one necessary new variable

          echo "-> Generating a dynamic scope file from the raw chunk..."
          dsieve -if "$CHUNK_FILE_PATH" -f 2 | sort -u > "$PARENT_DOMAINS_SCOPE"
          echo "  -> Scope for this chunk contains $(wc -l < "$PARENT_DOMAINS_SCOPE") parent domains."

          echo "-> Running puredns..."
          puredns resolve "$CHUNK_FILE_PATH" \
            -r resolvers.txt \
            --rate-limit 3000 \
            --skip-validation \
            --skip-wildcard-filter \
            --write "$PUREDNS_FILE" \
            --write-massdns "$MASSDNS" \
            --quiet >/dev/null 2>&1

          if [ ! -s "$MASSDNS" ]; then
              echo "INFO: Puredns found no resolvable hosts in this chunk. Exiting."
              mkdir -p results
              exit 0
          fi

          # ====================================================================
          # STAGE 2: CLEAN AND FILTER MASS_DNS DATA (Logic and Naming now match reference)
          # ====================================================================
          TMP_CLEANMASSDNS=$(mktemp)

          echo "-> Cleaning raw massdns output ('$MASSDNS')..."
          awk 'NF { sub(/\.$/,"",$1); print }' "$MASSDNS" > "$TMP_CLEANMASSDNS"

          echo "-> Filtering against the dynamic scope to create '$MASSDNS_FILE'..."
          awk '
          {gsub(/\r$/,"");sub(/^[ \t]+/,"");sub(/[ \t]+$/,"")}
          FNR==NR{if($0)patterns[++c]=$0;next}
          !setup{regex="";for(i=1;i<=c;i++){regex=regex (i>1?"|":"") "("patterns[i]")"};if(regex=="")regex="^\b$";setup=1}
          $2=="A" && $1~regex
          ' "$PARENT_DOMAINS_SCOPE" "$TMP_CLEANMASSDNS" | anew -q "$MASSDNS_FILE"
          echo "  -> Found $(wc -l < "$MASSDNS_FILE") in-scope A records."

          # ====================================================================
          # STAGE 3: MAP SUBDOMAINS TO PORTS (Logic and Naming now match reference)
          # ====================================================================
          OUTPUT="subdomain_ports.txt"
          SMAP_FILE="smap.txt"
          TMP_IP2SUB=$(mktemp)
          TMP_IP_ONLY=$(mktemp)
          TMP_NONCDN=$(mktemp)
          TMP_CDN=$(mktemp)
          TMP_SMAP_NONCDN=$(mktemp)
          TMP_RUSTSCAN=$(mktemp)

          echo "-> Extracting IPs from '$MASSDNS_FILE'..."
          awk '{ print $3, $1 }' "$MASSDNS_FILE" | sort -k1,1 > "$TMP_IP2SUB"
          cut -d' ' -f1 "$TMP_IP2SUB" | sort -u > "$TMP_IP_ONLY"

          echo "-> Filtering out CDN IPs..."
          cat "$TMP_IP_ONLY" | cut-cdn -ua -t 50 -silent -o "$TMP_NONCDN"
          cat "$TMP_IP_ONLY" | anew -d "$TMP_NONCDN" > "$TMP_CDN"

          echo "-> Scanning non-CDN IPs for ports 80, 443..."
          rustscan -a "$TMP_NONCDN" -p 70,80,81,82,83,84,85,88,443,888,1234,1311,2002,3000,3128,4443,5000,5222,5269,5800,7070,8000,8001,8002,8003,8004,8006,8008,8009,8010,8080,8081,8082,8083,8084,8085,8088,8181,8443,8880,8887,8888,9000,9001,9080,9090,9999,10000,10001,50000 --no-banner -t 1000 --tries 1 -u 20000 --scan-order "Random" -b 100 --greppable --accessible > "$TMP_RUSTSCAN" || true
          cat "$TMP_RUSTSCAN" | awk -F ' -> ' '{ gsub(/[\[\]]/, "", $2); n = split($2, p, ","); for(i=1;i<=n;i++) print $1 ":" p[i] }' | anew -q "$TMP_SMAP_NONCDN"

          cat "$TMP_SMAP_NONCDN" "$TMP_CDN" | sort -u > "$SMAP_FILE"

          echo "-> Joining IPs/Ports to create '$OUTPUT'..."
          awk -F: '
            NF==2 { print $1, $2 }
            NF==1 { print $1, ""  }
          ' "$SMAP_FILE" \
            | sort -k1,1 \
            | join - "$TMP_IP2SUB" \
            | {
              awk '
                NF >= 2 {
                  if (NF == 3 && $2 ~ /^[0-9]+$/) {
                    print $3 ":" $2
                  } else {
                    print $NF
                  }
                }
              '
            } \
            > "$OUTPUT"
          echo "  -> Generated enriched list with $(wc -l < "$OUTPUT") entries."

          # ====================================================================
          # STAGE 4: SORT FINAL RESULTS INTO ROOT DOMAIN FOLDERS
          # ====================================================================
          OUTPUT_ROOT="results"
          mkdir -p "$OUTPUT_ROOT"
          echo "-> Sorting all results into per-domain folders..."
          while read -r parent; do
            clean_parent=$(printf '%s' "$parent" | tr -d '\r' | xargs)
            if [ -z "$clean_parent" ]; then continue; fi
            mkdir -p "$OUTPUT_ROOT/$clean_parent"
            
            # Sort the simple puredns results (This file doesn't exist in the reference, but we keep it for extra data)
            simple_outfile="$OUTPUT_ROOT/$clean_parent/puredns_results.txt"
            grep -E "(^|\\.)${clean_parent//./\\.}(\$)" "$PUREDNS_FILE"  | anew -q "$simple_outfile" || true
            
            # Sort the enriched subdomain:port results
            ports_outfile="$OUTPUT_ROOT/$clean_parent/subdomain_ports.txt"
            grep -E "(^|\\.)${clean_parent//./\\.}(\:|\$)" "$OUTPUT" --color=never | anew -q "$ports_outfile" || true
          done < "$PARENT_DOMAINS_SCOPE"

          # ====================================================================
          # STAGE 5: CLEANUP (Matches reference)
          # ====================================================================
          rm -f "$TMP_CLEANMASSDNS" "$MASSDNS" "$PUREDNS_FILE"
          rm -f "$TMP_IP2SUB" "$TMP_IP_ONLY" "$TMP_NONCDN" "$TMP_CDN" "$TMP_SMAP_NONCDN" "$TMP_RUSTSCAN"

      - name: Compute SAFE_CHUNK (no slashes)
        run: |
          SAFE_CHUNK="${{ matrix.pair.chunk }}"
          SAFE_CHUNK="$(echo "$SAFE_CHUNK" | tr '/' '_')"
          echo "SAFE_CHUNK=$SAFE_CHUNK" >> $GITHUB_ENV

      - name: Upload Primary Account Results
        uses: actions/upload-artifact@v4
        with:
          name: recon-results-primary-${{ env.SAFE_CHUNK }}
          path: results/
          retention-days: 1
  
  commit_all_primary_results:
    name: Commit All Primary Results
    needs: resolve_primary_account_chunks
    if: always() 
    runs-on: ubuntu-latest
    container:
      image: ghcr.io/pcoder7/spider-puredns-actions:latest
      credentials:
        username: ${{ secrets.GHCR_USER }}
        password: ${{ secrets.GHCR_TOKEN }}        
    steps:
      - name: Checkout repository (Account 2's repo)
        uses: actions/checkout@v3
        with:
          token: ${{ secrets.GITHUB_TOKEN }}

      - name: Download all 'recon-results-primary-chunks_*' artifacts from this workflow run
        uses: actions/download-artifact@v4
        with:
          pattern: recon-results-primary-chunks_*
          path: temp_results

      - name: Organize and Push to store-recon
        shell: bash
        env:
          STORE_RECON_PAT: ${{ secrets.PAT_FOR_SECONDRY }}
          STORE: ${{ secrets.STORE }}
          ACCOUNT2_USERNAME: ${{ secrets.ACCOUNT2_REPO_OWNER }}
        run: |
          echo "Organizing primary results..."
          git config --global --add safe.directory "$(pwd)"
          # First, collect all chunk outputs into the runner’s results/ folder
          mkdir -p results
          find temp_results -type f -name "puredns_result.txt" | while read -r filepath; do
            parent="$(basename "$(dirname "$filepath")")"
            mkdir -p "results/$parent"
            cat "$filepath" | anew -q "results/$parent/puredns_result.txt"
            echo "WORKFLOW RUN: 'results/$parent/puredns_result.txt' contains $(wc -l < "results/$parent/puredns_result.txt") valid domains for parent $parent."
          done || true
          # If the runner’s results/ is empty, nothing to push
          if [ ! -d "results" ] || [ -z "$(ls -A results)" ]; then
            echo "No results to push."
            exit 0
          fi
          # Configure Git identity for commits
          git config --global user.name "Account2 PureDNS Bot"
          git config --global user.email "actions-bot@users.noreply.github.com"
          # Clone the private store-recon repo from Account 2
          TMP_DIR="$(mktemp -d)"
          echo "Cloning store-recon into $TMP_DIR"
          git clone "https://x-access-token:${STORE_RECON_PAT}@github.com/${ACCOUNT2_USERNAME}/${STORE}.git" "$TMP_DIR"
          cd "$TMP_DIR" || exit 1
          # Ensure a top-level results/ exists in the cloned repo
          mkdir -p results
          # Retry up to 10 times to handle races on main
          for i in {1..10}; do
            git fetch origin main
            git checkout main
            git pull --rebase origin main
                               
            
            WORKSPACE_RESULTS="${GITHUB_WORKSPACE}/results"
            find "$WORKSPACE_RESULTS" -type f -name "puredns_result.txt" | while read -r filepath; do
              rel="${filepath#"$WORKSPACE_RESULTS/"}"
              mkdir -p "results/$(dirname "$rel")"
              cat "$filepath" | anew -q "results/$rel"
              echo "DEBUG: 'results/$rel' now has $(wc -l < "results/$rel") lines."
            done
            git add results/
            if git diff --cached --quiet; then
              echo "No changes to commit in store-recon."
              exit 0
            fi
            git commit -m "Primary puredns+dsieve results (Run: ${{ github.event.inputs.primary_run_id || github.run_id }})"
            if git push origin main; then
              echo "Pushed to store-recon successfully on attempt #$i."
              exit 0
            else
              echo "Push failed on attempt #$i, retrying..."
              git reset --hard origin/main
            fi
          done
          echo "ERROR: Could not push results to store-recon after 10 attempts."
          exit 1      
      
